{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78HE8FLsKN9Q",
        "colab_type": "text"
      },
      "source": [
        "In this post, I take an in-depth look at word embeddings produced by Google's BERT and show you how to get started with BERT by producing your own word embeddings.\n",
        "\n",
        "This post is presented in two forms--as a blog post [here](http://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/) and as a Colab notebook [here](https://colab.research.google.com/drive/1yFphU6PW9Uo6lmDly_ud9a6c4RCYlwdX). \n",
        "The content is identical in both, but: \n",
        "\n",
        "* The blog post format may be easier to read, and includes a comments section for discussion. \n",
        "* The Colab Notebook will allow you to run the code and inspect it as you read through.\n",
        "\n",
        "*Update 5/27/20 - I've updated this post to use the new `transformers` library  from huggingface in place of the old `pytorch-pretrained-bert` library. You can still find the old post / Notebook [here](https://colab.research.google.com/drive/1ZQvuAVwA3IjybezQOXnrXMGAnMyZRuPU) if you need it.* \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYapTjoYa0kO",
        "colab_type": "text"
      },
      "source": [
        "# Introduction\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8HDKzBai5dL",
        "colab_type": "text"
      },
      "source": [
        "### History\n",
        "\n",
        "2018 was a breakthrough year in NLP. Transfer learning, particularly models like Allen AI's ELMO, OpenAI's Open-GPT, and Google's BERT allowed researchers to smash multiple benchmarks with minimal task-specific fine-tuning and provided the rest of the NLP community with pretrained models that could easily (with less data and less compute time) be fine-tuned and implemented to produce state of the art results. Unfortunately, for many starting out in NLP and even for some experienced practicioners, the theory and practical application of these powerful models is still not well understood.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WoitNQMWA1bt",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "### What is BERT?\n",
        "\n",
        "BERT (Bidirectional Encoder Representations from Transformers), released in late 2018, is the model we will use in this tutorial to provide readers with a better understanding of and practical guidance for using transfer learning models in NLP. BERT is a method of pretraining language representations that was used to create models that NLP practicioners can then download and use for free. You can either use these models to extract high quality language features from your text data, or you can fine-tune these models on a specific task (classification, entity recognition, question answering, etc.) with your own data to produce state of the art predictions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-dDVmXAA3At",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "### Why BERT embeddings?\n",
        "\n",
        "In this tutorial, we will use BERT to extract features, namely word and sentence embedding vectors, from text data. What can we do with these word and sentence embedding vectors? First, these embeddings are useful for keyword/search expansion, semantic search and information retrieval. For example, if you want to match customer questions or searches against already answered questions or well documented searches, these representations will help you accuratley retrieve results matching the customer's intent and contextual meaning, even if there's no keyword  or phrase overlap.\n",
        "\n",
        "Second, and perhaps more importantly, these vectors are used as high-quality feature inputs to downstream models. NLP models such as LSTMs or CNNs require inputs in the form of numerical vectors, and this typically means translating features like the vocabulary and parts of speech into numerical representations. In the past, words have been represented either as uniquely indexed values (one-hot encoding), or more helpfully as neural word embeddings where vocabulary words are matched against the fixed-length feature embeddings that result from models like Word2Vec or Fasttext. BERT offers an advantage over models like Word2Vec, because while each word has a fixed representation under Word2Vec regardless of the context within which the word appears, BERT produces word representations that are dynamically informed by the words around them. For example, given two sentences:\n",
        "\n",
        "\"The man was accused of robbing a bank.\"\n",
        "\"The man went fishing by the bank of the river.\"\n",
        "\n",
        "Word2Vec would produce the same word embedding for the word \"bank\" in both sentences, while under BERT the word embedding for \"bank\" would be different for each sentence. Aside from capturing obvious differences like polysemy, the context-informed word embeddings capture other forms of information that result in more accurate feature representations, which in turn results in better model performance.\n",
        "\n",
        "From an educational standpoint, a close examination of BERT word embeddings is a good way to get  your feet wet with BERT and its family of transfer learning models, and sets us up with some practical knowledge and context to better understand the inner details of the model in later tutorials.\n",
        "\n",
        "Onward!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pqa-7WXBAw8q",
        "colab_type": "text"
      },
      "source": [
        "# 1. Loading Pre-Trained BERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCdqJCtQN52l",
        "colab_type": "text"
      },
      "source": [
        "Install the pytorch interface for BERT by Hugging Face. (This library contains interfaces for other pretrained language models like OpenAI's GPT and GPT-2.) \n",
        "\n",
        "We've selected the pytorch interface because it strikes a nice balance between the high-level APIs (which are easy to use but don't provide insight into how things work) and tensorflow code (which contains lots of details but often sidetracks us into lessons about tensorflow, when the purpose here is BERT!).\n",
        "\n",
        "If you're running this code on Google Colab, you will have to install this library each time you reconnect; the following cell will take care of that for you."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RfUN_KolV-f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "outputId": "10e932f4-a95e-494f-8d3e-69a1e1be935c",
        "tags": []
      },
      "source": [
        "! pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Collecting transformers\n  Downloading transformers-3.0.2-py3-none-any.whl (769 kB)\n\u001b[K     |████████████████████████████████| 769 kB 904 kB/s \n\u001b[?25hCollecting tokenizers==0.8.1.rc1\n  Downloading tokenizers-0.8.1rc1-cp37-cp37m-macosx_10_10_x86_64.whl (2.1 MB)\n\u001b[K     |████████████████████████████████| 2.1 MB 14.9 MB/s \n\u001b[?25hCollecting sentencepiece!=0.1.92\n  Downloading sentencepiece-0.1.91-cp37-cp37m-macosx_10_6_x86_64.whl (1.1 MB)\n\u001b[K     |████████████████████████████████| 1.1 MB 16.0 MB/s \n\u001b[?25hCollecting regex!=2019.12.17\n  Downloading regex-2020.7.14.tar.gz (690 kB)\n\u001b[K     |████████████████████████████████| 690 kB 16.3 MB/s \n\u001b[?25hRequirement already satisfied: numpy in /Users/baeyuna/opt/anaconda3/lib/python3.7/site-packages (from transformers) (1.19.0)\nRequirement already satisfied: requests in /Users/baeyuna/opt/anaconda3/lib/python3.7/site-packages (from transformers) (2.22.0)\nRequirement already satisfied: filelock in /Users/baeyuna/opt/anaconda3/lib/python3.7/site-packages (from transformers) (3.0.12)\nRequirement already satisfied: packaging in /Users/baeyuna/opt/anaconda3/lib/python3.7/site-packages (from transformers) (20.1)\nRequirement already satisfied: tqdm>=4.27 in /Users/baeyuna/opt/anaconda3/lib/python3.7/site-packages (from transformers) (4.42.1)\nCollecting sacremoses\n  Downloading sacremoses-0.0.43.tar.gz (883 kB)\n\u001b[K     |████████████████████████████████| 883 kB 17.5 MB/s \n\u001b[?25hRequirement already satisfied: idna<2.9,>=2.5 in /Users/baeyuna/opt/anaconda3/lib/python3.7/site-packages (from requests->transformers) (2.8)\nRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /Users/baeyuna/opt/anaconda3/lib/python3.7/site-packages (from requests->transformers) (3.0.4)\nRequirement already satisfied: certifi>=2017.4.17 in /Users/baeyuna/opt/anaconda3/lib/python3.7/site-packages (from requests->transformers) (2020.6.20)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/baeyuna/opt/anaconda3/lib/python3.7/site-packages (from requests->transformers) (1.25.8)\nRequirement already satisfied: six in /Users/baeyuna/opt/anaconda3/lib/python3.7/site-packages (from packaging->transformers) (1.14.0)\nRequirement already satisfied: pyparsing>=2.0.2 in /Users/baeyuna/opt/anaconda3/lib/python3.7/site-packages (from packaging->transformers) (2.4.6)\nRequirement already satisfied: click in /Users/baeyuna/opt/anaconda3/lib/python3.7/site-packages (from sacremoses->transformers) (7.0)\nRequirement already satisfied: joblib in /Users/baeyuna/opt/anaconda3/lib/python3.7/site-packages (from sacremoses->transformers) (0.14.1)\nBuilding wheels for collected packages: regex, sacremoses\n  Building wheel for regex (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for regex: filename=regex-2020.7.14-cp37-cp37m-macosx_10_9_x86_64.whl size=286543 sha256=e95cd308de68f1d3b334cfc03a0b7455f349f5b73b57264e8f1d822878a7d2e7\n  Stored in directory: /Users/baeyuna/Library/Caches/pip/wheels/5b/68/ce/2508b5a5afc13bd96566c62d3ffebea7b401477c2ead3e8cc0\n  Building wheel for sacremoses (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for sacremoses: filename=sacremoses-0.0.43-py3-none-any.whl size=893259 sha256=98bc402be7007f7ded423602bca06b9e66459abe8ff4637b845f52d7c948072e\n  Stored in directory: /Users/baeyuna/Library/Caches/pip/wheels/69/09/d1/bf058f7d6fa0ecba2ce7c66be3b8d012beb4bf61a6e0c101c0\nSuccessfully built regex sacremoses\nInstalling collected packages: tokenizers, sentencepiece, regex, sacremoses, transformers\nSuccessfully installed regex-2020.7.14 sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.1rc1 transformers-3.0.2\n"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSXImOxMPdNg",
        "colab_type": "text"
      },
      "source": [
        "Now let's import pytorch, the pretrained BERT model, and a BERT tokenizer. \n",
        "\n",
        "We'll explain the BERT model in detail in a later tutorial, but this is the pre-trained model released by Google that ran for many, many hours on Wikipedia and [Book Corpus](https://arxiv.org/pdf/1506.06724.pdf), a dataset containing +10,000 books of different genres. This model is responsible (with a little modification) for beating NLP benchmarks across a range of tasks. Google released a few variations of BERT models, but the one we'll use here is the smaller of the two available sizes (\"base\" and \"large\") and ignores casing, hence \"uncased.\"\"\n",
        "\n",
        "`transformers` provides a number of classes for applying BERT to different tasks (token classification, text classification, ...). Here, we're using the basic `BertModel` which has no specific output task--it's a good choice for using BERT just to extract embeddings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJEnBJ3gHTsQ",
        "colab_type": "code",
        "colab": {},
        "tags": []
      },
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
        "import logging\n",
        "#logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "# % matplotlib inline\n",
        "\n",
        "# Load pre-trained model tokenizer (vocabulary)\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Downloading: 100%|██████████| 232k/232k [00:01<00:00, 161kB/s]\n"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tlv3VlPnKKHN",
        "colab_type": "text"
      },
      "source": [
        "# 2. Input Formatting\n",
        "Because BERT is a pretrained model that expects input data in a specific format, we will need:\n",
        "\n",
        "1. A **special token, `[SEP]`,** to mark the end of a sentence, or the separation between two sentences\n",
        "2. A **special token, `[CLS]`,** at the beginning of our text. This token is used for classification tasks, but BERT expects it no matter what your application is. \n",
        "3. Tokens that conform with the fixed vocabulary used in BERT\n",
        "4. The **Token IDs** for the tokens, from BERT's tokenizer\n",
        "5. **Mask IDs** to indicate which elements in the sequence are tokens and which are padding elements\n",
        "6. **Segment IDs** used to distinguish different sentences\n",
        "7. **Positional Embeddings** used to show token position within the sequence\n",
        "\n",
        "Luckily, the `transformers` interface takes care of all of the above requirements (using the `tokenizer.encode_plus` function). \n",
        "\n",
        "Since this is intended as an introduction to working with BERT, though, we're going to perform these steps in a (mostly) manual way. \n",
        "\n",
        "> *For an example of using `tokenizer.encode_plus`, see the next post on Sentence Classification [here](http://mccormickml.com/2019/07/22/BERT-fine-tuning/).*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "diVtyCJCurxJ",
        "colab_type": "text"
      },
      "source": [
        "## 2.1. Special Tokens\n",
        "BERT can take as input either one or two sentences, and uses the special token `[SEP]` to differentiate them. The `[CLS]` token always appears at the start of the text, and is specific to classification tasks. \n",
        "\n",
        "Both tokens are *always required*, however, even if we only have one sentence, and even if we are not using BERT for classification. That's how BERT was pre-trained, and so that's what BERT expects to see.\n",
        "\n",
        "**2 Sentence Input**:\n",
        "\n",
        "`[CLS] The man went to the store. [SEP] He bought a gallon of milk.`\n",
        "\n",
        "**1 Sentence Input**:\n",
        "\n",
        "`[CLS] The man went to the store. [SEP]`\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gsyrAwYvBfC",
        "colab_type": "text"
      },
      "source": [
        "## 2.2. Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WafgQPLAWmo",
        "colab_type": "text"
      },
      "source": [
        "BERT provides its own tokenizer, which we imported above. Let's see how it handles the below sentence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pg0P9rFxJwwp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a8314f8a-e95a-43e1-db4c-395f0334b942",
        "tags": []
      },
      "source": [
        "text = \"Here is the sentence I want embeddings for.\"\n",
        "marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
        "\n",
        "# Tokenize our sentence with the BERT tokenizer.\n",
        "tokenized_text = tokenizer.tokenize(marked_text)\n",
        "\n",
        "# Print out the tokens.\n",
        "print (tokenized_text)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "['[CLS]', 'here', 'is', 'the', 'sentence', 'i', 'want', 'em', '##bed', '##ding', '##s', 'for', '.', '[SEP]']\n"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q51eN4KAkbIJ",
        "colab_type": "text"
      },
      "source": [
        "Notice how the word \"embeddings\" is represented:\n",
        "\n",
        "`['em', '##bed', '##ding', '##s']`\n",
        "\n",
        "The original word has been split into smaller subwords and characters. The two hash signs preceding some of these subwords are just our tokenizer's way to denote that this subword or character is part of a larger word and preceded by another subword. So, for example, the '##bed' token is separate from the 'bed' token; the first is used whenever the subword 'bed' occurs within a larger word and the second is used explicitly for when the standalone token 'thing you sleep on' occurs.\n",
        "\n",
        "Why does it look this way? This is because the BERT tokenizer was created with a WordPiece model. This model greedily creates a fixed-size vocabulary of individual characters, subwords, and words that best fits our language data. Since the vocabulary limit size of our BERT tokenizer model is 30,000, the WordPiece model generated a vocabulary that contains all English characters plus the ~30,000 most common words and subwords found in the English language corpus the model is trained on. This vocabulary contains four things:\n",
        "\n",
        "1. Whole words\n",
        "2. Subwords occuring at the front of a word or in isolation (\"em\" as in \"embeddings\" is assigned the same vector as the standalone sequence of characters \"em\" as in \"go get em\" )\n",
        "3. Subwords not at the front of a word, which are preceded by '##' to denote this case\n",
        "4. Individual characters\n",
        "\n",
        "To tokenize a word under this model, the tokenizer first checks if the whole word is in the vocabulary. If not, it tries to break the word into the largest possible subwords contained in the vocabulary, and as a last resort will decompose the word into individual characters. Note that because of this, we can always represent a word as, at the very least, the collection of its individual characters.\n",
        "\n",
        "As a result, rather than assigning out of vocabulary words to a catch-all token like 'OOV' or 'UNK,' words that are not in the vocabulary are decomposed into subword and character tokens that we can then generate embeddings for. \n",
        "\n",
        "So, rather than assigning \"embeddings\" and every other out of vocabulary word to an overloaded unknown vocabulary token, we split it into subword tokens ['em', '##bed', '##ding', '##s'] that will retain some of the contextual meaning of the original word.  We can even average these subword embedding vectors to generate an approximate vector for the original word.\n",
        "\n",
        "\n",
        "(For more information about WordPiece, see the [original paper](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/37842.pdf) and further disucssion in Google's [Neural Machine Translation System](https://arxiv.org/pdf/1609.08144.pdf).)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jp5zXAPBVp82",
        "colab_type": "text"
      },
      "source": [
        "Here are some examples of the tokens contained in our vocabulary. Tokens beginning with two hashes are subwords or individual characters.\n",
        "\n",
        "*For an exploration of the contents of BERT's vocabulary, see [this notebook](https://colab.research.google.com/drive/1fCKIBJ6fgWQ-f6UKs7wDTpNTL9N-Cq9X) I created and the accompanying YouTube video [here](https://youtu.be/zJW57aCBCTk).*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1z1SzuTrqx-7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "outputId": "15c4ef58-a385-4650-bacb-83f0e4fd9eb4"
      },
      "source": [
        "list(tokenizer.vocab.keys())[5000:5020]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "['knight',\n 'lap',\n 'survey',\n 'ma',\n '##ow',\n 'noise',\n 'billy',\n '##ium',\n 'shooting',\n 'guide',\n 'bedroom',\n 'priest',\n 'resistance',\n 'motor',\n 'homes',\n 'sounded',\n 'giant',\n '##mer',\n '150',\n 'scenes']"
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HoF3LC47VgBb",
        "colab_type": "text"
      },
      "source": [
        "After breaking the text into tokens, we then have to convert the sentence from a list of strings to a list of vocabulary indeces.\n",
        "\n",
        "From here on, we'll use the below example sentence, which contains two instances of the word \"bank\" with different meanings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYjcYJuXoAQx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "outputId": "206d9627-954b-4e2b-8d31-cb31c49d267a",
        "tags": []
      },
      "source": [
        "# Define a new example sentence with multiple meanings of the word \"bank\"\n",
        "text = \"After stealing money from the bank vault, the bank robber was seen \" \\\n",
        "       \"fishing on the Mississippi river bank.\"\n",
        "\n",
        "# Add the special tokens.\n",
        "marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
        "\n",
        "# Split the sentence into tokens.\n",
        "tokenized_text = tokenizer.tokenize(marked_text)\n",
        "\n",
        "# Map the token strings to their vocabulary indeces.\n",
        "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "\n",
        "# Display the words with their indeces.\n",
        "for tup in zip(tokenized_text, indexed_tokens):\n",
        "    print('{:<12} {:>6,}'.format(tup[0], tup[1]))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "[CLS]           101\nafter         2,044\nstealing     11,065\nmoney         2,769\nfrom          2,013\nthe           1,996\nbank          2,924\nvault        11,632\n,             1,010\nthe           1,996\nbank          2,924\nrobber       27,307\nwas           2,001\nseen          2,464\nfishing       5,645\non            2,006\nthe           1,996\nmississippi   5,900\nriver         2,314\nbank          2,924\n.             1,012\n[SEP]           102\n"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "if6C_iCULU60",
        "colab_type": "text"
      },
      "source": [
        "## 2.3. Segment ID\n",
        "BERT is trained on and expects sentence pairs, using 1s and 0s to distinguish between the two sentences. That is, for each token in \"tokenized_text,\" we must specify which sentence it belongs to: sentence 0 (a series of 0s) or sentence 1 (a series of 1s). For our purposes, single-sentence inputs only require a series of 1s, so we will create a vector of 1s for each token in our input sentence. \n",
        "\n",
        "If you want to process two sentences, assign each word in the first sentence plus the '[SEP]' token a 0, and all tokens of the second sentence a 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_jEkVKxJMc0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "48869b82-0953-4557-c36c-6066582196bf",
        "tags": []
      },
      "source": [
        "# Mark each of the 22 tokens as belonging to sentence \"1\".\n",
        "segments_ids = [1] * len(tokenized_text)\n",
        "\n",
        "print (segments_ids)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-nY9LASLr2L",
        "colab_type": "text"
      },
      "source": [
        "# 3. Extracting Embeddings \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sl-iCj8wMEd5",
        "colab_type": "text"
      },
      "source": [
        "## 3.1. Running BERT on our text\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Nvaw46mfc8M",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Next we need to convert our data to torch tensors and call the BERT model. The BERT PyTorch interface requires that the data be in torch tensors rather than Python lists, so we convert the lists here - this does not change the shape or the data.\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_t4cM6KLc98",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert inputs to PyTorch tensors\n",
        "tokens_tensor = torch.tensor([indexed_tokens])\n",
        "segments_tensors = torch.tensor([segments_ids])"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCIGe0AXfg4Z",
        "colab_type": "text"
      },
      "source": [
        "Calling `from_pretrained` will fetch the model from the internet. When we load the `bert-base-uncased`, we see the definition of the model printed in the logging. The model is a deep neural network with 12 layers! Explaining the layers and their functions is outside the scope of this post, and you can skip over this output for now.\n",
        "\n",
        "model.eval() puts our model in evaluation mode as opposed to training mode. In this case, evaluation mode turns off dropout regularization which is used in training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mq2PKplWfbFv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "883fc897-22df-446b-f14c-d60f8a734f41",
        "tags": []
      },
      "source": [
        "# Load pre-trained model (weights)\n",
        "model = BertModel.from_pretrained('bert-base-uncased',\n",
        "                                  output_hidden_states = True, # Whether the model returns all hidden-states.\n",
        "                                  )\n",
        "\n",
        "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
        "model.eval()\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Downloading: 100%|██████████| 433/433 [00:00<00:00, 82.3kB/s]\nDownloading: 100%|██████████| 440M/440M [00:26<00:00, 16.6MB/s]\n"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "BertModel(\n  (embeddings): BertEmbeddings(\n    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n    (position_embeddings): Embedding(512, 768)\n    (token_type_embeddings): Embedding(2, 768)\n    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (encoder): BertEncoder(\n    (layer): ModuleList(\n      (0): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (1): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (2): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (3): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (4): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (5): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (6): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (7): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (8): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (9): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (10): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (11): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n  )\n  (pooler): BertPooler(\n    (dense): Linear(in_features=768, out_features=768, bias=True)\n    (activation): Tanh()\n  )\n)"
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4Qa5KkkM2Aq",
        "colab_type": "text"
      },
      "source": [
        "Next, let's evaluate BERT on our example text, and fetch the hidden states of the network!\n",
        "\n",
        "*Side note: `torch.no_grad` tells PyTorch not to construct the compute graph during this forward pass (since we won't be running backprop here)--this just reduces memory consumption and speeds things up a little.*\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nN0QTZwiMzeq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Run the text through BERT, and collect all of the hidden states produced\n",
        "# from all 12 layers. \n",
        "with torch.no_grad():\n",
        "\n",
        "    outputs = model(tokens_tensor, segments_tensors)\n",
        "\n",
        "    # Evaluating the model will return a different number of objects based on \n",
        "    # how it's  configured in the `from_pretrained` call earlier. In this case, \n",
        "    # becase we set `output_hidden_states = True`, the third item will be the \n",
        "    # hidden states from all layers. See the documentation for more details:\n",
        "    # https://huggingface.co/transformers/model_doc/bert.html#bertmodel\n",
        "    hidden_states = outputs[2]"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UeQNEFbUgMSf",
        "colab_type": "text"
      },
      "source": [
        "## 3.2. Understanding the Output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKTlTS_sfuAe",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "The full set of hidden states for this model, stored in the object `hidden_states`, is a little dizzying. This object has four dimensions, in the following order:\n",
        "\n",
        "1. The layer number (13 layers)\n",
        "2. The batch number (1 sentence)\n",
        "3. The word / token number (22 tokens in our sentence)\n",
        "4. The hidden unit / feature number (768 features)\n",
        "\n",
        "Wait, 13 layers? Doesn't BERT only have 12? It's 13 because the first element is the input embeddings, the rest is the outputs of each of BERT's 12 layers. \n",
        "\n",
        "That’s 219,648 unique values just to represent our one sentence! \n",
        "\n",
        "The second dimension, the batch size, is used when submitting multiple sentences to the model at once; here, though, we just have one example sentence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eI_uxiW7eRWA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "9a3e9695-7880-4396-8765-c5abce3510b8",
        "tags": []
      },
      "source": [
        "print (\"Number of layers:\", len(hidden_states), \"  (initial embeddings + 12 BERT layers)\")\n",
        "layer_i = 0\n",
        "\n",
        "print (\"Number of batches:\", len(hidden_states[layer_i]))\n",
        "batch_i = 0\n",
        "\n",
        "print (\"Number of tokens:\", len(hidden_states[layer_i][batch_i]))\n",
        "token_i = 0\n",
        "\n",
        "print (\"Number of hidden units:\", len(hidden_states[layer_i][batch_i][token_i]))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Number of layers: 13   (initial embeddings + 12 BERT layers)\nNumber of batches: 1\nNumber of tokens: 22\nNumber of hidden units: 768\n"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Uc_S_hmOWe7",
        "colab_type": "text"
      },
      "source": [
        "Let's take a quick look at the range of values for a given layer and token.\n",
        "\n",
        "You'll find that the range is fairly similar for all layers and tokens, with the majority of values falling between \\[-2, 2\\], and a small smattering of values around -10."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-UF_OAO-S1sP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 592
        },
        "outputId": "f16f8728-860c-4d9f-af67-95126e900b51"
      },
      "source": [
        "# For the 5th token in our sentence, select its feature values from layer 5.\n",
        "token_i = 5\n",
        "layer_i = 5\n",
        "vec = hidden_states[layer_i][batch_i][token_i]\n",
        "\n",
        "# Plot the values as a histogram to show their distribution.\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.hist(vec, bins=200)\n",
        "plt.show()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<Figure size 720x720 with 1 Axes>",
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"574.678125pt\" version=\"1.1\" viewBox=\"0 0 592.125 574.678125\" width=\"592.125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <defs>\n  <style type=\"text/css\">\n*{stroke-linecap:butt;stroke-linejoin:round;}\n  </style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 574.678125 \nL 592.125 574.678125 \nL 592.125 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 26.925 550.8 \nL 584.925 550.8 \nL 584.925 7.2 \nL 26.925 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 52.288636 550.8 \nL 54.824986 550.8 \nL 54.824986 541.717293 \nL 52.288636 541.717293 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 54.824986 550.8 \nL 57.361368 550.8 \nL 57.361368 550.8 \nL 54.824986 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 57.361401 550.8 \nL 59.89775 550.8 \nL 59.89775 550.8 \nL 57.361401 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 59.897717 550.8 \nL 62.434099 550.8 \nL 62.434099 550.8 \nL 59.897717 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_7\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 62.434099 550.8 \nL 64.970448 550.8 \nL 64.970448 550.8 \nL 62.434099 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_8\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 64.970448 550.8 \nL 67.50683 550.8 \nL 67.50683 550.8 \nL 64.970448 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_9\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 67.506863 550.8 \nL 70.043212 550.8 \nL 70.043212 550.8 \nL 67.506863 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_10\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 70.043179 550.8 \nL 72.579562 550.8 \nL 72.579562 550.8 \nL 70.043179 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_11\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 72.579562 550.8 \nL 75.115911 550.8 \nL 75.115911 550.8 \nL 72.579562 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_12\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 75.115944 550.8 \nL 77.652293 550.8 \nL 77.652293 550.8 \nL 75.115944 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_13\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 77.65226 550.8 \nL 80.188642 550.8 \nL 80.188642 550.8 \nL 77.65226 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_14\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 80.188642 550.8 \nL 82.724991 550.8 \nL 82.724991 550.8 \nL 80.188642 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_15\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 82.724991 550.8 \nL 85.261373 550.8 \nL 85.261373 550.8 \nL 82.724991 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_16\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 85.261406 550.8 \nL 87.797755 550.8 \nL 87.797755 550.8 \nL 85.261406 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_17\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 87.797723 550.8 \nL 90.334105 550.8 \nL 90.334105 550.8 \nL 87.797723 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_18\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 90.334105 550.8 \nL 92.870454 550.8 \nL 92.870454 550.8 \nL 90.334105 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_19\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 92.870487 550.8 \nL 95.406836 550.8 \nL 95.406836 550.8 \nL 92.870487 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_20\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 95.406803 550.8 \nL 97.943185 550.8 \nL 97.943185 550.8 \nL 95.406803 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_21\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 97.943185 550.8 \nL 100.479534 550.8 \nL 100.479534 550.8 \nL 97.943185 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_22\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 100.479534 550.8 \nL 103.015916 550.8 \nL 103.015916 550.8 \nL 100.479534 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_23\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 103.015949 550.8 \nL 105.552299 550.8 \nL 105.552299 550.8 \nL 103.015949 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_24\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 105.552266 550.8 \nL 108.088648 550.8 \nL 108.088648 550.8 \nL 105.552266 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_25\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 108.088648 550.8 \nL 110.624997 550.8 \nL 110.624997 550.8 \nL 108.088648 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_26\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 110.62503 550.8 \nL 113.161379 550.8 \nL 113.161379 550.8 \nL 110.62503 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_27\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 113.161346 550.8 \nL 115.697728 550.8 \nL 115.697728 550.8 \nL 113.161346 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_28\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 115.697728 550.8 \nL 118.234078 550.8 \nL 118.234078 550.8 \nL 115.697728 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_29\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 118.234078 550.8 \nL 120.77046 550.8 \nL 120.77046 550.8 \nL 118.234078 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_30\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 120.770492 550.8 \nL 123.306842 550.8 \nL 123.306842 550.8 \nL 120.770492 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_31\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 123.306809 550.8 \nL 125.843191 550.8 \nL 125.843191 550.8 \nL 123.306809 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_32\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 125.843191 550.8 \nL 128.37954 550.8 \nL 128.37954 550.8 \nL 125.843191 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_33\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 128.37954 550.8 \nL 130.915922 550.8 \nL 130.915922 550.8 \nL 128.37954 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_34\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 130.915955 550.8 \nL 133.452304 550.8 \nL 133.452304 550.8 \nL 130.915955 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_35\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 133.452271 550.8 \nL 135.988621 550.8 \nL 135.988621 550.8 \nL 133.452271 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_36\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 135.988621 550.8 \nL 138.525003 550.8 \nL 138.525003 550.8 \nL 135.988621 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_37\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 138.525036 550.8 \nL 141.061385 550.8 \nL 141.061385 550.8 \nL 138.525036 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_38\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 141.061352 550.8 \nL 143.597734 550.8 \nL 143.597734 550.8 \nL 141.061352 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_39\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 143.597734 550.8 \nL 146.134083 550.8 \nL 146.134083 550.8 \nL 143.597734 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_40\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 146.134083 550.8 \nL 148.670465 550.8 \nL 148.670465 550.8 \nL 146.134083 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_41\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 148.670498 550.8 \nL 151.206847 550.8 \nL 151.206847 550.8 \nL 148.670498 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_42\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 151.206815 550.8 \nL 153.743197 550.8 \nL 153.743197 550.8 \nL 151.206815 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_43\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 153.743197 550.8 \nL 156.279546 550.8 \nL 156.279546 550.8 \nL 153.743197 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_44\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 156.279579 550.8 \nL 158.815928 550.8 \nL 158.815928 550.8 \nL 156.279579 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_45\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 158.815895 550.8 \nL 161.352277 550.8 \nL 161.352277 550.8 \nL 158.815895 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_46\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 161.352277 550.8 \nL 163.888626 550.8 \nL 163.888626 550.8 \nL 161.352277 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_47\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 163.888626 550.8 \nL 166.425008 550.8 \nL 166.425008 550.8 \nL 163.888626 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_48\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 166.425041 550.8 \nL 168.96139 550.8 \nL 168.96139 550.8 \nL 166.425041 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_49\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 168.961358 550.8 \nL 171.49774 550.8 \nL 171.49774 550.8 \nL 168.961358 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_50\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 171.49774 550.8 \nL 174.034089 550.8 \nL 174.034089 550.8 \nL 171.49774 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_51\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 174.034122 550.8 \nL 176.570471 550.8 \nL 176.570471 550.8 \nL 174.034122 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_52\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 176.570438 550.8 \nL 179.10682 550.8 \nL 179.10682 550.8 \nL 176.570438 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_53\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 179.10682 550.8 \nL 181.643169 550.8 \nL 181.643169 550.8 \nL 179.10682 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_54\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 181.643169 550.8 \nL 184.179552 550.8 \nL 184.179552 550.8 \nL 181.643169 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_55\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 184.179584 550.8 \nL 186.715934 550.8 \nL 186.715934 550.8 \nL 184.179584 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_56\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 186.715901 550.8 \nL 189.252283 550.8 \nL 189.252283 550.8 \nL 186.715901 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_57\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 189.252283 550.8 \nL 191.788632 550.8 \nL 191.788632 550.8 \nL 189.252283 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_58\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 191.788665 550.8 \nL 194.325014 550.8 \nL 194.325014 550.8 \nL 191.788665 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_59\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 194.324981 550.8 \nL 196.861363 550.8 \nL 196.861363 550.8 \nL 194.324981 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_60\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 196.861363 550.8 \nL 199.397713 550.8 \nL 199.397713 550.8 \nL 196.861363 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_61\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 199.397713 550.8 \nL 201.934095 550.8 \nL 201.934095 550.8 \nL 199.397713 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_62\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 201.934128 550.8 \nL 204.470477 550.8 \nL 204.470477 550.8 \nL 201.934128 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_63\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 204.470444 550.8 \nL 207.006826 550.8 \nL 207.006826 550.8 \nL 204.470444 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_64\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 207.006826 550.8 \nL 209.543175 550.8 \nL 209.543175 550.8 \nL 207.006826 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_65\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 209.543175 550.8 \nL 212.079557 550.8 \nL 212.079557 550.8 \nL 209.543175 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_66\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 212.07959 550.8 \nL 214.615939 550.8 \nL 214.615939 550.8 \nL 212.07959 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_67\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 214.615906 550.8 \nL 217.152256 550.8 \nL 217.152256 550.8 \nL 214.615906 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_68\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 217.152256 550.8 \nL 219.688638 550.8 \nL 219.688638 550.8 \nL 217.152256 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_69\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 219.688671 550.8 \nL 222.22502 550.8 \nL 222.22502 550.8 \nL 219.688671 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_70\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 222.224987 550.8 \nL 224.761369 550.8 \nL 224.761369 550.8 \nL 222.224987 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_71\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 224.761369 550.8 \nL 227.297718 550.8 \nL 227.297718 550.8 \nL 224.761369 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_72\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 227.297718 550.8 \nL 229.8341 550.8 \nL 229.8341 550.8 \nL 227.297718 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_73\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 229.8341 550.8 \nL 232.37045 550.8 \nL 232.37045 550.8 \nL 229.8341 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_74\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 232.37045 550.8 \nL 234.906832 550.8 \nL 234.906832 550.8 \nL 232.37045 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_75\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 234.906799 550.8 \nL 237.443181 550.8 \nL 237.443181 550.8 \nL 234.906799 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_76\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 237.443181 550.8 \nL 239.979563 550.8 \nL 239.979563 550.8 \nL 237.443181 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_77\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 239.97953 550.8 \nL 242.515912 550.8 \nL 242.515912 550.8 \nL 239.97953 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_78\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 242.515912 550.8 \nL 245.052261 550.8 \nL 245.052261 550.8 \nL 242.515912 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_79\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 245.052261 550.8 \nL 247.588643 550.8 \nL 247.588643 550.8 \nL 245.052261 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_80\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 247.588611 550.8 \nL 250.124993 550.8 \nL 250.124993 550.8 \nL 247.588611 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_81\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 250.124993 550.8 \nL 252.661375 550.8 \nL 252.661375 550.8 \nL 250.124993 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_82\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 252.661342 550.8 \nL 255.197724 550.8 \nL 255.197724 550.8 \nL 252.661342 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_83\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 255.197724 550.8 \nL 257.734106 550.8 \nL 257.734106 550.8 \nL 255.197724 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_84\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 257.734073 550.8 \nL 260.270455 550.8 \nL 260.270455 550.8 \nL 257.734073 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_85\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 260.270455 550.8 \nL 262.806837 550.8 \nL 262.806837 550.8 \nL 260.270455 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_86\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 262.806805 550.8 \nL 265.343187 550.8 \nL 265.343187 550.8 \nL 262.806805 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_87\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 265.343187 550.8 \nL 267.879536 550.8 \nL 267.879536 550.8 \nL 265.343187 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_88\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 267.879536 550.8 \nL 270.415918 550.8 \nL 270.415918 550.8 \nL 267.879536 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_89\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 270.415885 550.8 \nL 272.952267 550.8 \nL 272.952267 550.8 \nL 270.415885 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_90\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 272.952267 550.8 \nL 275.488649 550.8 \nL 275.488649 550.8 \nL 272.952267 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_91\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 275.488616 550.8 \nL 278.024998 550.8 \nL 278.024998 550.8 \nL 275.488616 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_92\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 278.024998 550.8 \nL 280.56138 550.8 \nL 280.56138 550.8 \nL 278.024998 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_93\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 280.561348 550.8 \nL 283.09773 550.8 \nL 283.09773 550.8 \nL 280.561348 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_94\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 283.09773 550.8 \nL 285.634079 550.8 \nL 285.634079 550.8 \nL 283.09773 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_95\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 285.634079 550.8 \nL 288.170461 550.8 \nL 288.170461 550.8 \nL 285.634079 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_96\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 288.170428 550.8 \nL 290.70681 550.8 \nL 290.70681 550.8 \nL 288.170428 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_97\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 290.70681 550.8 \nL 293.243192 550.8 \nL 293.243192 550.8 \nL 290.70681 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_98\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 293.243159 550.8 \nL 295.779542 550.8 \nL 295.779542 550.8 \nL 293.243159 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_99\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 295.779542 550.8 \nL 298.315924 550.8 \nL 298.315924 550.8 \nL 295.779542 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_100\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 298.315891 550.8 \nL 300.852273 550.8 \nL 300.852273 550.8 \nL 298.315891 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_101\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 300.852273 550.8 \nL 303.388655 550.8 \nL 303.388655 550.8 \nL 300.852273 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_102\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 303.388622 550.8 \nL 305.925004 550.8 \nL 305.925004 550.8 \nL 303.388622 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_103\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 305.925004 550.8 \nL 308.461353 550.8 \nL 308.461353 550.8 \nL 305.925004 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_104\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 308.461353 550.8 \nL 310.997735 550.8 \nL 310.997735 550.8 \nL 308.461353 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_105\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 310.997703 550.8 \nL 313.534085 550.8 \nL 313.534085 550.8 \nL 310.997703 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_106\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 313.534085 550.8 \nL 316.070467 550.8 \nL 316.070467 550.8 \nL 313.534085 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_107\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 316.070434 550.8 \nL 318.606816 550.8 \nL 318.606816 550.8 \nL 316.070434 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_108\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 318.606816 550.8 \nL 321.143198 550.8 \nL 321.143198 550.8 \nL 318.606816 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_109\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 321.143165 550.8 \nL 323.679547 550.8 \nL 323.679547 550.8 \nL 321.143165 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_110\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 323.679547 550.8 \nL 326.215896 550.8 \nL 326.215896 550.8 \nL 323.679547 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_111\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 326.215896 550.8 \nL 328.752279 550.8 \nL 328.752279 550.8 \nL 326.215896 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_112\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 328.752246 550.8 \nL 331.288628 550.8 \nL 331.288628 550.8 \nL 328.752246 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_113\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 331.288628 550.8 \nL 333.82501 550.8 \nL 333.82501 550.8 \nL 331.288628 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_114\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 333.824977 550.8 \nL 336.361359 550.8 \nL 336.361359 550.8 \nL 333.824977 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_115\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 336.361359 550.8 \nL 338.897741 550.8 \nL 338.897741 550.8 \nL 336.361359 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_116\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 338.897708 550.8 \nL 341.43409 550.8 \nL 341.43409 550.8 \nL 338.897708 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_117\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 341.43409 550.8 \nL 343.970472 550.8 \nL 343.970472 550.8 \nL 341.43409 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_118\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 343.97044 550.8 \nL 346.506822 550.8 \nL 346.506822 550.8 \nL 343.97044 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_119\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 346.506822 550.8 \nL 349.043171 550.8 \nL 349.043171 550.8 \nL 346.506822 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_120\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 349.043171 550.8 \nL 351.579553 550.8 \nL 351.579553 550.8 \nL 349.043171 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_121\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 351.57952 550.8 \nL 354.115902 550.8 \nL 354.115902 550.8 \nL 351.57952 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_122\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 354.115902 550.8 \nL 356.652284 550.8 \nL 356.652284 550.8 \nL 354.115902 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_123\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 356.652251 550.8 \nL 359.188633 550.8 \nL 359.188633 550.8 \nL 356.652251 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_124\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 359.188633 550.8 \nL 361.725016 550.8 \nL 361.725016 550.8 \nL 359.188633 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_125\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 361.724983 550.8 \nL 364.261365 550.8 \nL 364.261365 550.8 \nL 361.724983 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_126\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 364.261365 550.8 \nL 366.797714 550.8 \nL 366.797714 550.8 \nL 364.261365 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_127\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 366.797714 550.8 \nL 369.334096 550.8 \nL 369.334096 550.8 \nL 366.797714 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_128\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 369.334096 550.8 \nL 371.870445 550.8 \nL 371.870445 550.8 \nL 369.334096 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_129\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 371.870437 550.8 \nL 374.406819 550.8 \nL 374.406819 550.8 \nL 371.870437 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_130\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 374.406803 550.8 \nL 376.943185 550.8 \nL 376.943185 550.8 \nL 374.406803 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_131\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 376.943168 550.8 \nL 379.47955 550.8 \nL 379.47955 550.8 \nL 376.943168 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_132\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 379.479534 550.8 \nL 382.015916 550.8 \nL 382.015916 550.8 \nL 379.479534 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_133\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 382.015908 550.8 \nL 384.552257 550.8 \nL 384.552257 550.8 \nL 382.015908 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_134\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 384.552257 550.8 \nL 387.088639 550.8 \nL 387.088639 550.8 \nL 384.552257 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_135\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 387.088623 550.8 \nL 389.625005 550.8 \nL 389.625005 550.8 \nL 387.088623 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_136\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 389.625005 550.8 \nL 392.161354 550.8 \nL 392.161354 550.8 \nL 389.625005 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_137\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 392.161346 550.8 \nL 394.697728 550.8 \nL 394.697728 550.8 \nL 392.161346 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_138\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 394.697711 550.8 \nL 397.234094 550.8 \nL 397.234094 550.8 \nL 394.697711 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_139\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 397.234077 550.8 \nL 399.770459 550.8 \nL 399.770459 550.8 \nL 397.234077 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_140\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 399.770443 550.8 \nL 402.306825 550.8 \nL 402.306825 550.8 \nL 399.770443 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_141\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 402.306817 550.8 \nL 404.843166 550.8 \nL 404.843166 550.8 \nL 402.306817 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_142\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 404.843166 550.8 \nL 407.379548 550.8 \nL 407.379548 550.8 \nL 404.843166 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_143\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 407.379531 550.8 \nL 409.915914 550.8 \nL 409.915914 550.8 \nL 407.379531 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_144\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 409.915914 550.8 \nL 412.452263 550.8 \nL 412.452263 550.8 \nL 409.915914 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_145\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 412.452255 550.8 \nL 414.988637 550.8 \nL 414.988637 550.8 \nL 412.452255 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_146\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 414.98862 550.8 \nL 417.525002 550.8 \nL 417.525002 550.8 \nL 414.98862 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_147\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 417.524986 550.8 \nL 420.061368 550.8 \nL 420.061368 550.8 \nL 417.524986 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_148\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 420.061352 550.8 \nL 422.597734 550.8 \nL 422.597734 550.8 \nL 420.061352 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_149\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 422.597725 550.8 \nL 425.134075 550.8 \nL 425.134075 550.8 \nL 422.597725 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_150\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 425.134075 550.8 \nL 427.670457 550.8 \nL 427.670457 550.8 \nL 425.134075 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_151\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 427.67044 550.8 \nL 430.206822 550.8 \nL 430.206822 550.8 \nL 427.67044 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_152\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 430.206822 550.8 \nL 432.743172 550.8 \nL 432.743172 550.8 \nL 430.206822 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_153\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 432.743163 550.8 \nL 435.279545 550.8 \nL 435.279545 550.8 \nL 432.743163 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_154\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 435.279529 550.8 \nL 437.815911 550.8 \nL 437.815911 550.8 \nL 435.279529 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_155\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 437.815895 550.8 \nL 440.352277 550.8 \nL 440.352277 550.8 \nL 437.815895 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_156\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 440.352277 550.8 \nL 442.888626 550.8 \nL 442.888626 550.8 \nL 440.352277 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_157\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 442.888634 550.8 \nL 445.424983 550.8 \nL 445.424983 550.8 \nL 442.888634 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_158\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 445.424983 550.8 \nL 447.961365 550.8 \nL 447.961365 550.8 \nL 445.424983 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_159\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 447.961349 550.8 \nL 450.497731 550.8 \nL 450.497731 550.8 \nL 447.961349 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_160\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 450.497731 550.8 \nL 453.03408 550.8 \nL 453.03408 550.8 \nL 450.497731 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_161\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 453.034089 550.8 \nL 455.570438 550.8 \nL 455.570438 550.8 \nL 453.034089 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_162\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 455.570438 550.8 \nL 458.10682 550.8 \nL 458.10682 541.717293 \nL 455.570438 541.717293 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_163\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 458.106803 550.8 \nL 460.643185 550.8 \nL 460.643185 532.634586 \nL 458.106803 532.634586 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_164\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 460.643185 550.8 \nL 463.179535 550.8 \nL 463.179535 541.717293 \nL 460.643185 541.717293 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_165\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 463.179543 550.8 \nL 465.715892 550.8 \nL 465.715892 487.221053 \nL 463.179543 487.221053 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_166\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 465.715892 550.8 \nL 468.252274 550.8 \nL 468.252274 514.469173 \nL 465.715892 514.469173 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_167\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 468.252258 550.8 \nL 470.78864 550.8 \nL 470.78864 532.634586 \nL 468.252258 532.634586 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_168\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 470.78864 550.8 \nL 473.324989 550.8 \nL 473.324989 469.055639 \nL 470.78864 469.055639 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_169\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 473.325001 550.8 \nL 475.861351 550.8 \nL 475.861351 441.807519 \nL 473.325001 441.807519 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_170\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 475.861363 550.8 \nL 478.397712 550.8 \nL 478.397712 441.807519 \nL 475.861363 441.807519 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_171\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 478.397729 550.8 \nL 480.934078 550.8 \nL 480.934078 441.807519 \nL 478.397729 441.807519 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_172\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 480.93409 550.8 \nL 483.470439 550.8 \nL 483.470439 432.724812 \nL 480.93409 432.724812 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_173\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 483.470456 550.8 \nL 486.006805 550.8 \nL 486.006805 350.980451 \nL 483.470456 350.980451 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_174\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 486.006817 550.8 \nL 488.543167 550.8 \nL 488.543167 323.732331 \nL 486.006817 323.732331 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_175\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 488.543182 550.8 \nL 491.079531 550.8 \nL 491.079531 332.815038 \nL 488.543182 332.815038 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_176\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 491.079546 550.8 \nL 493.615895 550.8 \nL 493.615895 232.905263 \nL 491.079546 232.905263 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_177\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 493.615909 550.8 \nL 496.152258 550.8 \nL 496.152258 187.491729 \nL 493.615909 187.491729 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_178\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 496.152273 550.8 \nL 498.688622 550.8 \nL 498.688622 205.657143 \nL 496.152273 205.657143 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_179\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 498.688636 550.8 \nL 501.224986 550.8 \nL 501.224986 33.085714 \nL 498.688636 33.085714 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_180\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 501.225 550.8 \nL 503.761349 550.8 \nL 503.761349 69.416541 \nL 501.225 69.416541 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_181\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 503.761364 550.8 \nL 506.297713 550.8 \nL 506.297713 160.243609 \nL 503.761364 160.243609 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_182\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 506.297727 550.8 \nL 508.834076 550.8 \nL 508.834076 169.326316 \nL 506.297727 169.326316 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_183\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 508.834091 550.8 \nL 511.37044 550.8 \nL 511.37044 160.243609 \nL 508.834091 160.243609 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_184\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 511.370454 550.8 \nL 513.906804 550.8 \nL 513.906804 214.73985 \nL 511.370454 214.73985 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_185\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 513.906818 550.8 \nL 516.443167 550.8 \nL 516.443167 178.409023 \nL 513.906818 178.409023 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_186\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 516.443181 550.8 \nL 518.979531 550.8 \nL 518.979531 214.73985 \nL 516.443181 214.73985 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_187\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 518.979545 550.8 \nL 521.515894 550.8 \nL 521.515894 278.318797 \nL 518.979545 278.318797 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_188\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 521.515908 550.8 \nL 524.052257 550.8 \nL 524.052257 369.145865 \nL 521.515908 369.145865 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_189\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 524.052273 550.8 \nL 526.588623 550.8 \nL 526.588623 387.311278 \nL 524.052273 387.311278 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_190\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 526.588635 550.8 \nL 529.124984 550.8 \nL 529.124984 387.311278 \nL 526.588635 387.311278 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_191\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 529.125 550.8 \nL 531.66135 550.8 \nL 531.66135 432.724812 \nL 529.125 432.724812 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_192\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 531.661362 550.8 \nL 534.197711 550.8 \nL 534.197711 441.807519 \nL 531.661362 441.807519 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_193\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 534.197728 550.8 \nL 536.734077 550.8 \nL 536.734077 441.807519 \nL 534.197728 441.807519 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_194\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 536.734089 550.8 \nL 539.270438 550.8 \nL 539.270438 478.138346 \nL 536.734089 478.138346 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_195\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 539.270438 550.8 \nL 541.806821 550.8 \nL 541.806821 496.303759 \nL 539.270438 496.303759 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_196\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 541.806821 550.8 \nL 544.34317 550.8 \nL 544.34317 523.55188 \nL 541.806821 523.55188 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_197\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 544.343178 550.8 \nL 546.879527 550.8 \nL 546.879527 505.386466 \nL 544.343178 505.386466 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_198\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 546.879527 550.8 \nL 549.415909 550.8 \nL 549.415909 514.469173 \nL 546.879527 514.469173 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_199\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 549.415893 550.8 \nL 551.952275 550.8 \nL 551.952275 550.8 \nL 549.415893 550.8 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_200\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 551.952275 550.8 \nL 554.488624 550.8 \nL 554.488624 532.634586 \nL 551.952275 532.634586 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_201\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 554.488632 550.8 \nL 557.024982 550.8 \nL 557.024982 523.55188 \nL 554.488632 523.55188 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_202\">\n    <path clip-path=\"url(#pe4cfd80ef1)\" d=\"M 557.024982 550.8 \nL 559.561364 550.8 \nL 559.561364 541.717293 \nL 557.024982 541.717293 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"mb0246088d2\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"92.130355\" xlink:href=\"#mb0246088d2\" y=\"550.8\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- −12 -->\n      <defs>\n       <path d=\"M 10.59375 35.5 \nL 73.1875 35.5 \nL 73.1875 27.203125 \nL 10.59375 27.203125 \nz\n\" id=\"DejaVuSans-8722\"/>\n       <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n       <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n      </defs>\n      <g transform=\"translate(81.578012 565.398438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-8722\"/>\n       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"147.412109\" xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"161.02985\" xlink:href=\"#mb0246088d2\" y=\"550.8\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- −10 -->\n      <defs>\n       <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n      </defs>\n      <g transform=\"translate(150.477506 565.398438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-8722\"/>\n       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"147.412109\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"229.929344\" xlink:href=\"#mb0246088d2\" y=\"550.8\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- −8 -->\n      <defs>\n       <path d=\"M 31.78125 34.625 \nQ 24.75 34.625 20.71875 30.859375 \nQ 16.703125 27.09375 16.703125 20.515625 \nQ 16.703125 13.921875 20.71875 10.15625 \nQ 24.75 6.390625 31.78125 6.390625 \nQ 38.8125 6.390625 42.859375 10.171875 \nQ 46.921875 13.96875 46.921875 20.515625 \nQ 46.921875 27.09375 42.890625 30.859375 \nQ 38.875 34.625 31.78125 34.625 \nz\nM 21.921875 38.8125 \nQ 15.578125 40.375 12.03125 44.71875 \nQ 8.5 49.078125 8.5 55.328125 \nQ 8.5 64.0625 14.71875 69.140625 \nQ 20.953125 74.21875 31.78125 74.21875 \nQ 42.671875 74.21875 48.875 69.140625 \nQ 55.078125 64.0625 55.078125 55.328125 \nQ 55.078125 49.078125 51.53125 44.71875 \nQ 48 40.375 41.703125 38.8125 \nQ 48.828125 37.15625 52.796875 32.3125 \nQ 56.78125 27.484375 56.78125 20.515625 \nQ 56.78125 9.90625 50.3125 4.234375 \nQ 43.84375 -1.421875 31.78125 -1.421875 \nQ 19.734375 -1.421875 13.25 4.234375 \nQ 6.78125 9.90625 6.78125 20.515625 \nQ 6.78125 27.484375 10.78125 32.3125 \nQ 14.796875 37.15625 21.921875 38.8125 \nz\nM 18.3125 54.390625 \nQ 18.3125 48.734375 21.84375 45.5625 \nQ 25.390625 42.390625 31.78125 42.390625 \nQ 38.140625 42.390625 41.71875 45.5625 \nQ 45.3125 48.734375 45.3125 54.390625 \nQ 45.3125 60.0625 41.71875 63.234375 \nQ 38.140625 66.40625 31.78125 66.40625 \nQ 25.390625 66.40625 21.84375 63.234375 \nQ 18.3125 60.0625 18.3125 54.390625 \nz\n\" id=\"DejaVuSans-56\"/>\n      </defs>\n      <g transform=\"translate(222.55825 565.398438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-8722\"/>\n       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-56\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"298.828838\" xlink:href=\"#mb0246088d2\" y=\"550.8\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- −6 -->\n      <defs>\n       <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n      </defs>\n      <g transform=\"translate(291.457744 565.398438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-8722\"/>\n       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-54\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"367.728332\" xlink:href=\"#mb0246088d2\" y=\"550.8\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- −4 -->\n      <defs>\n       <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n      </defs>\n      <g transform=\"translate(360.357238 565.398438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-8722\"/>\n       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-52\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"436.627826\" xlink:href=\"#mb0246088d2\" y=\"550.8\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- −2 -->\n      <g transform=\"translate(429.256732 565.398438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-8722\"/>\n       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"505.52732\" xlink:href=\"#mb0246088d2\" y=\"550.8\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 0 -->\n      <g transform=\"translate(502.34607 565.398438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_8\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"574.426814\" xlink:href=\"#mb0246088d2\" y=\"550.8\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 2 -->\n      <g transform=\"translate(571.245564 565.398438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_9\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m2fc6d84aac\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m2fc6d84aac\" y=\"550.8\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 0 -->\n      <g transform=\"translate(13.5625 554.599219)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m2fc6d84aac\" y=\"459.972932\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 10 -->\n      <g transform=\"translate(7.2 463.772151)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m2fc6d84aac\" y=\"369.145865\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 20 -->\n      <g transform=\"translate(7.2 372.945083)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m2fc6d84aac\" y=\"278.318797\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 30 -->\n      <defs>\n       <path d=\"M 40.578125 39.3125 \nQ 47.65625 37.796875 51.625 33 \nQ 55.609375 28.21875 55.609375 21.1875 \nQ 55.609375 10.40625 48.1875 4.484375 \nQ 40.765625 -1.421875 27.09375 -1.421875 \nQ 22.515625 -1.421875 17.65625 -0.515625 \nQ 12.796875 0.390625 7.625 2.203125 \nL 7.625 11.71875 \nQ 11.71875 9.328125 16.59375 8.109375 \nQ 21.484375 6.890625 26.8125 6.890625 \nQ 36.078125 6.890625 40.9375 10.546875 \nQ 45.796875 14.203125 45.796875 21.1875 \nQ 45.796875 27.640625 41.28125 31.265625 \nQ 36.765625 34.90625 28.71875 34.90625 \nL 20.21875 34.90625 \nL 20.21875 43.015625 \nL 29.109375 43.015625 \nQ 36.375 43.015625 40.234375 45.921875 \nQ 44.09375 48.828125 44.09375 54.296875 \nQ 44.09375 59.90625 40.109375 62.90625 \nQ 36.140625 65.921875 28.71875 65.921875 \nQ 24.65625 65.921875 20.015625 65.03125 \nQ 15.375 64.15625 9.8125 62.3125 \nL 9.8125 71.09375 \nQ 15.4375 72.65625 20.34375 73.4375 \nQ 25.25 74.21875 29.59375 74.21875 \nQ 40.828125 74.21875 47.359375 69.109375 \nQ 53.90625 64.015625 53.90625 55.328125 \nQ 53.90625 49.265625 50.4375 45.09375 \nQ 46.96875 40.921875 40.578125 39.3125 \nz\n\" id=\"DejaVuSans-51\"/>\n      </defs>\n      <g transform=\"translate(7.2 282.118016)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_13\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m2fc6d84aac\" y=\"187.491729\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 40 -->\n      <g transform=\"translate(7.2 191.290948)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m2fc6d84aac\" y=\"96.664662\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 50 -->\n      <defs>\n       <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n      </defs>\n      <g transform=\"translate(7.2 100.46388)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_203\">\n    <path d=\"M 26.925 550.8 \nL 26.925 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_204\">\n    <path d=\"M 584.925 550.8 \nL 584.925 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_205\">\n    <path d=\"M 26.925 550.8 \nL 584.925 550.8 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_206\">\n    <path d=\"M 26.925 7.2 \nL 584.925 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"pe4cfd80ef1\">\n   <rect height=\"543.6\" width=\"558\" x=\"26.925\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n",
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "n194RcReDYfw"
      },
      "source": [
        "Grouping the values by layer makes sense for the model, but for our purposes we want it grouped by token. \n",
        "\n",
        "Current dimensions:\n",
        "\n",
        "`[# layers, # batches, # tokens, # features]`\n",
        "\n",
        "Desired dimensions:\n",
        "\n",
        "`[# tokens, # layers, # features]`\n",
        "\n",
        "Luckily, PyTorch includes the `permute` function for easily rearranging the dimensions of a tensor. \n",
        "\n",
        "However, the first dimension is currently a Python list! "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0CcY_oRwcHlS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "3be557cf-e149-426e-caa9-766c31149403",
        "tags": []
      },
      "source": [
        "# `hidden_states` is a Python list.\n",
        "print('      Type of hidden_states: ', type(hidden_states))\n",
        "\n",
        "# Each layer in the list is a torch tensor.\n",
        "print('Tensor shape for each layer: ', hidden_states[0].size())"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Type of hidden_states:  <class 'tuple'>\nTensor shape for each layer:  torch.Size([1, 22, 768])\n"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1yXZjLSke3F0",
        "colab_type": "text"
      },
      "source": [
        "Let's combine the layers to make this one whole big tensor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTJV8AFFcLbL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "05334078-7859-4fa5-e09d-2ee191d89566"
      },
      "source": [
        "# Concatenate the tensors for all layers. We use `stack` here to\n",
        "# create a new dimension in the tensor.\n",
        "token_embeddings = torch.stack(hidden_states, dim=0)\n",
        "\n",
        "token_embeddings.size()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "torch.Size([13, 1, 22, 768])"
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnBv2TUNhzf4",
        "colab_type": "text"
      },
      "source": [
        "Let's get rid of the \"batches\" dimension since we don't need it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "En4JZ41fh6CI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "172213e4-222d-4bb2-c3d0-cd1fb6509943"
      },
      "source": [
        "# Remove dimension 1, the \"batches\".\n",
        "token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
        "\n",
        "token_embeddings.size()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "torch.Size([13, 22, 768])"
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVzRfvkbe-Yp",
        "colab_type": "text"
      },
      "source": [
        "Finally, we can switch around the \"layers\" and \"tokens\" dimensions with `permute`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtDVE58cdeYp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2bd2b3e9-341b-46bd-d3c1-c54b2cab8230"
      },
      "source": [
        "# Swap dimensions 0 and 1.\n",
        "token_embeddings = token_embeddings.permute(1,0,2)\n",
        "\n",
        "token_embeddings.size()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([22, 13, 768])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ey5RhOQ7NGtz",
        "colab_type": "text"
      },
      "source": [
        "## 3.3. Creating word and sentence vectors from hidden states\n",
        "\n",
        "Now, what do we do with these hidden states? We would like to get individual vectors for each of our tokens, or perhaps a single vector representation of the whole sentence, but for each token of our input we have 13 separate vectors each of length 768.\n",
        "\n",
        "In order to get the individual vectors we will need to combine some of the layer vectors...but which layer or combination of layers provides the best representation? \n",
        "\n",
        "Unfortunately, there's no single easy answer... Let's try a couple reasonable approaches, though. Afterwards, I'll point you to some helpful resources which look into this question further.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76TdtFH8NM9q",
        "colab_type": "text"
      },
      "source": [
        "### Word Vectors\n",
        "\n",
        "To give you some examples, let's create word vectors two ways. \n",
        "\n",
        "First, let's **concatenate** the last four layers, giving us a single word vector per token. Each vector will have length `4 x 768 = 3,072`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pv42h9jANMRf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "095760f6-a5b7-48ed-d687-506871e1f3e3",
        "tags": []
      },
      "source": [
        "# Stores the token vectors, with shape [22 x 3,072]\n",
        "token_vecs_cat = []\n",
        "\n",
        "# `token_embeddings` is a [22 x 12 x 768] tensor.\n",
        "\n",
        "# For each token in the sentence...\n",
        "for token in token_embeddings:\n",
        "    \n",
        "    # `token` is a [12 x 768] tensor\n",
        "\n",
        "    # Concatenate the vectors (that is, append them together) from the last \n",
        "    # four layers.\n",
        "    # Each layer vector is 768 values, so `cat_vec` is length 3,072.\n",
        "    cat_vec = torch.cat((token[-1], token[-2], token[-3], token[-4]), dim=0)\n",
        "    \n",
        "    # Use `cat_vec` to represent `token`.\n",
        "    token_vecs_cat.append(cat_vec)\n",
        "\n",
        "print ('Shape is: %d x %d' % (len(token_vecs_cat), len(token_vecs_cat[0])))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Shape is: 13 x 3072\n"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VnWaByfelM-e",
        "colab_type": "text"
      },
      "source": [
        "As an alternative method, let's try creating the word vectors by **summing** together the last four layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4DKDtFwiF0S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e29f8073-61c0-451d-bdd2-398d99ddfc4d",
        "tags": []
      },
      "source": [
        "# Stores the token vectors, with shape [22 x 768]\n",
        "token_vecs_sum = []\n",
        "\n",
        "# `token_embeddings` is a [22 x 12 x 768] tensor.\n",
        "\n",
        "# For each token in the sentence...\n",
        "for token in token_embeddings:\n",
        "\n",
        "    # `token` is a [12 x 768] tensor\n",
        "\n",
        "    # Sum the vectors from the last four layers.\n",
        "    sum_vec = torch.sum(token[-4:], dim=0)\n",
        "    \n",
        "    # Use `sum_vec` to represent `token`.\n",
        "    token_vecs_sum.append(sum_vec)\n",
        "\n",
        "print ('Shape is: %d x %d' % (len(token_vecs_sum), len(token_vecs_sum[0])))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Shape is: 13 x 768\n"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQaco6jRLkXn",
        "colab_type": "text"
      },
      "source": [
        "### Sentence Vectors\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uuul6iQqnXT2",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "To get a single vector for our entire sentence we have multiple application-dependent strategies, but a simple approach is to average the second to last hiden layer of each token producing a single 768 length vector."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zn0n2S-FWZih",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# `hidden_states` has shape [13 x 1 x 22 x 768]\n",
        "\n",
        "# `token_vecs` is a tensor with shape [22 x 768]\n",
        "token_vecs = hidden_states[-2][0]\n",
        "\n",
        "# Calculate the average of all 22 token vectors.\n",
        "sentence_embedding = torch.mean(token_vecs, dim=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQv0FL8VWadn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9fce910b-f15c-4545-fc0e-8d75e656f237"
      },
      "source": [
        "print (\"Our final sentence embedding vector of shape:\", sentence_embedding.size())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Our final sentence embedding vector of shape: torch.Size([768])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TqYcrAipfE3E",
        "colab_type": "text"
      },
      "source": [
        "## 3.4. Confirming contextually dependent vectors\n",
        "\n",
        "To confirm that the value of these vectors are in fact contextually dependent, let's look at the different instances of the word \"bank\" in our example sentence:\n",
        "\n",
        "\"After stealing money from the **bank vault**, the **bank robber** was seen fishing on the Mississippi **river bank**.\"\n",
        "\n",
        "Let's find the index of those three instances of the word \"bank\" in the example sentence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNiRsEh9cmWz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "outputId": "7dfa1ea6-cd85-4094-a701-de0a21914298"
      },
      "source": [
        "for i, token_str in enumerate(tokenized_text):\n",
        "  print (i, token_str)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 [CLS]\n",
            "1 after\n",
            "2 stealing\n",
            "3 money\n",
            "4 from\n",
            "5 the\n",
            "6 bank\n",
            "7 vault\n",
            "8 ,\n",
            "9 the\n",
            "10 bank\n",
            "11 robber\n",
            "12 was\n",
            "13 seen\n",
            "14 fishing\n",
            "15 on\n",
            "16 the\n",
            "17 mississippi\n",
            "18 river\n",
            "19 bank\n",
            "20 .\n",
            "21 [SEP]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEhBIA5RlS8-",
        "colab_type": "text"
      },
      "source": [
        "They are at 6, 10, and 19.\n",
        "\n",
        "For this analysis, we'll use the word vectors that we created by summing the last four layers.\n",
        "\n",
        "We can try printing out their vectors to compare them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBa6vRHknSkv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "outputId": "8d5a547a-d9dc-4f1d-fcea-ecdd84f73f9d"
      },
      "source": [
        "print('First 5 vector values for each instance of \"bank\".')\n",
        "print('')\n",
        "print(\"bank vault   \", str(token_vecs_sum[6][:5]))\n",
        "print(\"bank robber  \", str(token_vecs_sum[10][:5]))\n",
        "print(\"river bank   \", str(token_vecs_sum[19][:5]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First 5 vector values for each instance of \"bank\".\n",
            "\n",
            "bank vault    tensor([ 3.3596, -2.9805, -1.5421,  0.7065,  2.0031])\n",
            "bank robber   tensor([ 2.7359, -2.5577, -1.3094,  0.6797,  1.6633])\n",
            "river bank    tensor([ 1.5266, -0.8895, -0.5152, -0.9298,  2.8334])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ca2TCQ_G7SM3",
        "colab_type": "text"
      },
      "source": [
        "We can see that the values differ, but let's calculate the cosine similarity between the vectors to make a more precise comparison."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYXUwiG0yhBS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "38d3962f-59af-4ece-d380-e140f524380d"
      },
      "source": [
        "from scipy.spatial.distance import cosine\n",
        "\n",
        "# Calculate the cosine similarity between the word bank \n",
        "# in \"bank robber\" vs \"river bank\" (different meanings).\n",
        "diff_bank = 1 - cosine(token_vecs_sum[10], token_vecs_sum[19])\n",
        "\n",
        "# Calculate the cosine similarity between the word bank\n",
        "# in \"bank robber\" vs \"bank vault\" (same meaning).\n",
        "same_bank = 1 - cosine(token_vecs_sum[10], token_vecs_sum[6])\n",
        "\n",
        "print('Vector similarity for  *similar*  meanings:  %.2f' % same_bank)\n",
        "print('Vector similarity for *different* meanings:  %.2f' % diff_bank)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vector similarity for  *similar*  meanings:  0.94\n",
            "Vector similarity for *different* meanings:  0.69\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7jroXfKspe_",
        "colab_type": "text"
      },
      "source": [
        "This looks pretty good!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orjhWUJgmxo5",
        "colab_type": "text"
      },
      "source": [
        "## 3.5. Pooling Strategy & Layer Choice"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1CI97kNn8dD",
        "colab_type": "text"
      },
      "source": [
        "Below are a couple additional resources for exploring this topic."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3D5qnRNmq5_",
        "colab_type": "text"
      },
      "source": [
        "**BERT Authors**\n",
        "\n",
        "The BERT authors tested word-embedding strategies by feeding different vector combinations as input features to a BiLSTM used on a named entity recognition task and observing the resulting F1 scores.\n",
        "\n",
        "(Image from [Jay Allamar](http://jalammar.github.io/illustrated-bert/)'s blog)\n",
        "\n",
        "\n",
        "![alt text](http://jalammar.github.io/images/bert-feature-extraction-contextualized-embeddings.png)\n",
        "\n",
        "While concatenation of the last four layers produced the best results on this specific task, many of the other methods come in a close second and in general it is advisable to test different versions for your specific application: results may vary.\n",
        "\n",
        "This is partially demonstrated by noting that the different layers of BERT encode very different kinds of information, so the appropriate pooling strategy will change depending on the application because different layers encode different kinds of information. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7_CVgejm5pr",
        "colab_type": "text"
      },
      "source": [
        "**Han Xiao's BERT-as-service**\n",
        "\n",
        "Han Xiao created an open-source project named [bert-as-service](https://github.com/hanxiao/bert-as-service) on GitHub which is intended to create word embeddings for your text using BERT. Han experimented with different approaches to combining these embeddings, and shared some conclusions and rationale on the [FAQ page](https://github.com/hanxiao/bert-as-service#speech_balloon-faq) of the project. \n",
        "\n",
        "`bert-as-service`, by default, uses the outputs from the **second-to-last layer** of the model. \n",
        "\n",
        "I would summarize Han's perspective by the following:\n",
        "\n",
        "1. The embeddings start out in the first layer as having no contextual information (i.e., the meaning of the initial 'bank' embedding isn't specific to river bank or financial bank).\n",
        "2. As the embeddings move deeper into the network, they pick up more and more contextual information with each layer.\n",
        "3. As you approach the final layer, however, you start picking up information that is specific to BERT's pre-training tasks (the \"Masked Language Model\" (MLM) and \"Next Sentence Prediction\" (NSP)). \n",
        "    * What we want is embeddings that encode the word meaning well... \n",
        "    * BERT is motivated to do this, but it is also motivated to encode anything else that would help it determine what a missing word is (MLM), or whether the second sentence came after the first (NSP). \n",
        "4. The second-to-last layer is what Han settled on as a reasonable sweet-spot.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONLJ36JfPuqf",
        "colab_type": "text"
      },
      "source": [
        "# 4. Appendix\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdw7cLJWMr_Y",
        "colab_type": "text"
      },
      "source": [
        "## 4.1. Special tokens\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jyx2kQxbnHbM",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "It should be noted that although the `[CLS]` acts as an \"aggregate representation\" for classification tasks, this is not the best choice for a high quality sentence embedding vector. [According to](https://github.com/google-research/bert/issues/164) BERT author Jacob Devlin: \"*I'm not sure what these vectors are, since BERT does not generate meaningful sentence vectors. It seems that this is is doing average pooling over the word tokens to get a sentence vector, but we never suggested that this will generate meaningful sentence representations*.\"\n",
        "\n",
        "(However, the [CLS] token does become meaningful if the model has been fine-tuned, where the last hidden layer of this token is used as the \"sentence vector\" for sequence classification.)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EbS8_z6XMuTJ",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## 4.2. Out of vocabulary words\n",
        "\n",
        "For **out of vocabulary words** that are composed of multiple sentence and character-level embeddings, there is a further issue of how best to recover this embedding. Averaging the embeddings is the most straightforward solution (one that is relied upon in similar embedding models with subword vocabularies like fasttext), but summation of subword embeddings and simply taking the last token embedding (remember that the vectors are context sensitive) are acceptable alternative strategies.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BokW7CAgMxCB",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## 4.3. Similarity metrics\n",
        "\n",
        "It is worth noting that word-level **similarity comparisons** are not appropriate with BERT embeddings because these embeddings are contextually dependent, meaning that the word vector changes depending on the sentence it appears in. This allows wonderful things like polysemy so that e.g. your representation encodes river \"bank\" and not a financial institution \"bank\",  but makes direct word-to-word similarity comparisons less valuable. However, for sentence embeddings similarity comparison is still valid such that one can query, for example, a single sentence against a dataset of other sentences in order to find the most similar. Depending on the similarity metric used, the resulting similarity values will be less informative than the relative ranking of similarity outputs since many similarity metrics make assumptions about the vector space (equally-weighted dimensions, for example) that do not hold for our 768-dimensional vector space.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0unZ2xh4QDap",
        "colab_type": "text"
      },
      "source": [
        "## 4.4. Implementations\n",
        "\n",
        "You can use the code in this notebook as the foundation of your own application to extract BERT features from text. However, official [tensorflow](https://github.com/google-research/bert/blob/master/extract_features.py) and well-regarded [pytorch](https://github.com/huggingface/pytorch-pretrained-BERT/blob/master/examples/extract_features.py) implementations already exist that do this for you.  Additionally, [bert-as-a-service](https://github.com/hanxiao/bert-as-service) is an excellent tool designed specifically for running this task with high performance, and is the one I would recommend for production applications. The author has taken great care in the tool's implementation and provides excellent documentation (some of which was used to help create this tutorial) to help users understand the more nuanced details the user faces, like resource management and pooling strategy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OhbZxbKRxMvM",
        "colab_type": "text"
      },
      "source": [
        "## Cite\n",
        "Chris McCormick and Nick Ryan. (2019, May 14). *BERT Word Embeddings Tutorial*. Retrieved from http://www.mccormickml.com\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "BERT Word Embeddings v2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}