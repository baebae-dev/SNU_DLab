{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import model_name\n",
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from os import path\n",
    "import random\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import csv\n",
    "from pathlib import Path\n",
    "from shutil import copyfile\n",
    "import importlib\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "          0         1        2         3         4         5         6    \\\n,   -0.082752  0.672040 -0.14987 -0.064983  0.056491  0.402280  0.002775   \n.    0.012001  0.207510 -0.12578 -0.593250  0.125250  0.159750  0.137480   \nthe  0.272040 -0.062030 -0.18840  0.023225 -0.018158  0.006719 -0.138770   \nand -0.185670  0.066008 -0.25209 -0.117250  0.265130  0.064908  0.122910   \nto   0.319240  0.063160 -0.27858  0.261200  0.079248 -0.214620 -0.104950   \n\n          7         8       9    ...      290       291      292       293  \\\n,   -0.331100 -0.306910  2.0817  ... -0.14331  0.018267 -0.18643  0.207090   \n.   -0.331570 -0.136940  1.7893  ...  0.16165 -0.066737 -0.29556  0.022612   \nthe  0.177080  0.177090  2.5882  ... -0.42810  0.168990  0.22511 -0.285570   \nand -0.093979  0.024321  2.4926  ... -0.59396 -0.097729  0.20072  0.170550   \nto   0.154950 -0.033530  2.4834  ... -0.12977  0.371300  0.18888 -0.004274   \n\n          294       295       296       297       298      299  \n,   -0.355980  0.053380 -0.050821 -0.191800 -0.378460 -0.06589  \n.   -0.281350  0.063500  0.140190  0.138710 -0.360490 -0.03500  \nthe -0.102800 -0.018168  0.114070  0.130150 -0.183170  0.13230  \nand -0.004736 -0.039709  0.324980 -0.023452  0.123020  0.33120  \nto  -0.106450 -0.258100 -0.044629  0.082745  0.097801  0.25045  \n\n[5 rows x 300 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>290</th>\n      <th>291</th>\n      <th>292</th>\n      <th>293</th>\n      <th>294</th>\n      <th>295</th>\n      <th>296</th>\n      <th>297</th>\n      <th>298</th>\n      <th>299</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>,</th>\n      <td>-0.082752</td>\n      <td>0.672040</td>\n      <td>-0.14987</td>\n      <td>-0.064983</td>\n      <td>0.056491</td>\n      <td>0.402280</td>\n      <td>0.002775</td>\n      <td>-0.331100</td>\n      <td>-0.306910</td>\n      <td>2.0817</td>\n      <td>...</td>\n      <td>-0.14331</td>\n      <td>0.018267</td>\n      <td>-0.18643</td>\n      <td>0.207090</td>\n      <td>-0.355980</td>\n      <td>0.053380</td>\n      <td>-0.050821</td>\n      <td>-0.191800</td>\n      <td>-0.378460</td>\n      <td>-0.06589</td>\n    </tr>\n    <tr>\n      <th>.</th>\n      <td>0.012001</td>\n      <td>0.207510</td>\n      <td>-0.12578</td>\n      <td>-0.593250</td>\n      <td>0.125250</td>\n      <td>0.159750</td>\n      <td>0.137480</td>\n      <td>-0.331570</td>\n      <td>-0.136940</td>\n      <td>1.7893</td>\n      <td>...</td>\n      <td>0.16165</td>\n      <td>-0.066737</td>\n      <td>-0.29556</td>\n      <td>0.022612</td>\n      <td>-0.281350</td>\n      <td>0.063500</td>\n      <td>0.140190</td>\n      <td>0.138710</td>\n      <td>-0.360490</td>\n      <td>-0.03500</td>\n    </tr>\n    <tr>\n      <th>the</th>\n      <td>0.272040</td>\n      <td>-0.062030</td>\n      <td>-0.18840</td>\n      <td>0.023225</td>\n      <td>-0.018158</td>\n      <td>0.006719</td>\n      <td>-0.138770</td>\n      <td>0.177080</td>\n      <td>0.177090</td>\n      <td>2.5882</td>\n      <td>...</td>\n      <td>-0.42810</td>\n      <td>0.168990</td>\n      <td>0.22511</td>\n      <td>-0.285570</td>\n      <td>-0.102800</td>\n      <td>-0.018168</td>\n      <td>0.114070</td>\n      <td>0.130150</td>\n      <td>-0.183170</td>\n      <td>0.13230</td>\n    </tr>\n    <tr>\n      <th>and</th>\n      <td>-0.185670</td>\n      <td>0.066008</td>\n      <td>-0.25209</td>\n      <td>-0.117250</td>\n      <td>0.265130</td>\n      <td>0.064908</td>\n      <td>0.122910</td>\n      <td>-0.093979</td>\n      <td>0.024321</td>\n      <td>2.4926</td>\n      <td>...</td>\n      <td>-0.59396</td>\n      <td>-0.097729</td>\n      <td>0.20072</td>\n      <td>0.170550</td>\n      <td>-0.004736</td>\n      <td>-0.039709</td>\n      <td>0.324980</td>\n      <td>-0.023452</td>\n      <td>0.123020</td>\n      <td>0.33120</td>\n    </tr>\n    <tr>\n      <th>to</th>\n      <td>0.319240</td>\n      <td>0.063160</td>\n      <td>-0.27858</td>\n      <td>0.261200</td>\n      <td>0.079248</td>\n      <td>-0.214620</td>\n      <td>-0.104950</td>\n      <td>0.154950</td>\n      <td>-0.033530</td>\n      <td>2.4834</td>\n      <td>...</td>\n      <td>-0.12977</td>\n      <td>0.371300</td>\n      <td>0.18888</td>\n      <td>-0.004274</td>\n      <td>-0.106450</td>\n      <td>-0.258100</td>\n      <td>-0.044629</td>\n      <td>0.082745</td>\n      <td>0.097801</td>\n      <td>0.25045</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 300 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "source_embedding = pd.read_table('/Users/baeyuna/Documents/SNU_DLab/TANR_Bert/data/glove/glove.840B.300d.txt',\n",
    "                                     index_col=0,\n",
    "                                     sep=' ',\n",
    "                                     header=None,\n",
    "                                     quoting=csv.QUOTE_NONE,\n",
    "                                     names=range(300))\n",
    "source_embedding.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "           0         1        2         3         4         5         6    \\\nword                                                                        \n,    -0.082752  0.672040 -0.14987 -0.064983  0.056491  0.402280  0.002775   \n.     0.012001  0.207510 -0.12578 -0.593250  0.125250  0.159750  0.137480   \nthe   0.272040 -0.062030 -0.18840  0.023225 -0.018158  0.006719 -0.138770   \nand  -0.185670  0.066008 -0.25209 -0.117250  0.265130  0.064908  0.122910   \nto    0.319240  0.063160 -0.27858  0.261200  0.079248 -0.214620 -0.104950   \n\n           7         8       9    ...      290       291      292       293  \\\nword                              ...                                         \n,    -0.331100 -0.306910  2.0817  ... -0.14331  0.018267 -0.18643  0.207090   \n.    -0.331570 -0.136940  1.7893  ...  0.16165 -0.066737 -0.29556  0.022612   \nthe   0.177080  0.177090  2.5882  ... -0.42810  0.168990  0.22511 -0.285570   \nand  -0.093979  0.024321  2.4926  ... -0.59396 -0.097729  0.20072  0.170550   \nto    0.154950 -0.033530  2.4834  ... -0.12977  0.371300  0.18888 -0.004274   \n\n           294       295       296       297       298      299  \nword                                                             \n,    -0.355980  0.053380 -0.050821 -0.191800 -0.378460 -0.06589  \n.    -0.281350  0.063500  0.140190  0.138710 -0.360490 -0.03500  \nthe  -0.102800 -0.018168  0.114070  0.130150 -0.183170  0.13230  \nand  -0.004736 -0.039709  0.324980 -0.023452  0.123020  0.33120  \nto   -0.106450 -0.258100 -0.044629  0.082745  0.097801  0.25045  \n\n[5 rows x 300 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>290</th>\n      <th>291</th>\n      <th>292</th>\n      <th>293</th>\n      <th>294</th>\n      <th>295</th>\n      <th>296</th>\n      <th>297</th>\n      <th>298</th>\n      <th>299</th>\n    </tr>\n    <tr>\n      <th>word</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>,</th>\n      <td>-0.082752</td>\n      <td>0.672040</td>\n      <td>-0.14987</td>\n      <td>-0.064983</td>\n      <td>0.056491</td>\n      <td>0.402280</td>\n      <td>0.002775</td>\n      <td>-0.331100</td>\n      <td>-0.306910</td>\n      <td>2.0817</td>\n      <td>...</td>\n      <td>-0.14331</td>\n      <td>0.018267</td>\n      <td>-0.18643</td>\n      <td>0.207090</td>\n      <td>-0.355980</td>\n      <td>0.053380</td>\n      <td>-0.050821</td>\n      <td>-0.191800</td>\n      <td>-0.378460</td>\n      <td>-0.06589</td>\n    </tr>\n    <tr>\n      <th>.</th>\n      <td>0.012001</td>\n      <td>0.207510</td>\n      <td>-0.12578</td>\n      <td>-0.593250</td>\n      <td>0.125250</td>\n      <td>0.159750</td>\n      <td>0.137480</td>\n      <td>-0.331570</td>\n      <td>-0.136940</td>\n      <td>1.7893</td>\n      <td>...</td>\n      <td>0.16165</td>\n      <td>-0.066737</td>\n      <td>-0.29556</td>\n      <td>0.022612</td>\n      <td>-0.281350</td>\n      <td>0.063500</td>\n      <td>0.140190</td>\n      <td>0.138710</td>\n      <td>-0.360490</td>\n      <td>-0.03500</td>\n    </tr>\n    <tr>\n      <th>the</th>\n      <td>0.272040</td>\n      <td>-0.062030</td>\n      <td>-0.18840</td>\n      <td>0.023225</td>\n      <td>-0.018158</td>\n      <td>0.006719</td>\n      <td>-0.138770</td>\n      <td>0.177080</td>\n      <td>0.177090</td>\n      <td>2.5882</td>\n      <td>...</td>\n      <td>-0.42810</td>\n      <td>0.168990</td>\n      <td>0.22511</td>\n      <td>-0.285570</td>\n      <td>-0.102800</td>\n      <td>-0.018168</td>\n      <td>0.114070</td>\n      <td>0.130150</td>\n      <td>-0.183170</td>\n      <td>0.13230</td>\n    </tr>\n    <tr>\n      <th>and</th>\n      <td>-0.185670</td>\n      <td>0.066008</td>\n      <td>-0.25209</td>\n      <td>-0.117250</td>\n      <td>0.265130</td>\n      <td>0.064908</td>\n      <td>0.122910</td>\n      <td>-0.093979</td>\n      <td>0.024321</td>\n      <td>2.4926</td>\n      <td>...</td>\n      <td>-0.59396</td>\n      <td>-0.097729</td>\n      <td>0.20072</td>\n      <td>0.170550</td>\n      <td>-0.004736</td>\n      <td>-0.039709</td>\n      <td>0.324980</td>\n      <td>-0.023452</td>\n      <td>0.123020</td>\n      <td>0.33120</td>\n    </tr>\n    <tr>\n      <th>to</th>\n      <td>0.319240</td>\n      <td>0.063160</td>\n      <td>-0.27858</td>\n      <td>0.261200</td>\n      <td>0.079248</td>\n      <td>-0.214620</td>\n      <td>-0.104950</td>\n      <td>0.154950</td>\n      <td>-0.033530</td>\n      <td>2.4834</td>\n      <td>...</td>\n      <td>-0.12977</td>\n      <td>0.371300</td>\n      <td>0.18888</td>\n      <td>-0.004274</td>\n      <td>-0.106450</td>\n      <td>-0.258100</td>\n      <td>-0.044629</td>\n      <td>0.082745</td>\n      <td>0.097801</td>\n      <td>0.25045</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 300 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "source_embedding.index.rename('word', inplace=True)\n",
    "source_embedding.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<class 'pandas.core.frame.DataFrame'>\nIndex: 2196017 entries, , to zulchzulu\nColumns: 300 entries, 0 to 299\ndtypes: float64(300)\nmemory usage: 4.9+ GB\n"
    }
   ],
   "source": [
    "source_embedding.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                   int\nword                  \nthe                  1\nbrands               2\nqueen                3\nelizabeth            4\n,                    5\n...                ...\nkhizr            44771\nedwards-helaire  44772\nkorg             44773\nminilogue        44774\n78-55            44775\n\n[44775 rows x 1 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>int</th>\n    </tr>\n    <tr>\n      <th>word</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>the</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>brands</th>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>queen</th>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>elizabeth</th>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>,</th>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>khizr</th>\n      <td>44771</td>\n    </tr>\n    <tr>\n      <th>edwards-helaire</th>\n      <td>44772</td>\n    </tr>\n    <tr>\n      <th>korg</th>\n      <td>44773</td>\n    </tr>\n    <tr>\n      <th>minilogue</th>\n      <td>44774</td>\n    </tr>\n    <tr>\n      <th>78-55</th>\n      <td>44775</td>\n    </tr>\n  </tbody>\n</table>\n<p>44775 rows × 1 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "source": [
    "word2int = pd.read_table('/Users/baeyuna/Documents/SNU_DLab/NAML/data/train/word2int.tsv', na_filter=False, index_col='word')\n",
    "word2int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bert_word_embedding 으로 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[ 0.02291545,  0.01740933,  0.01044573, ...,  0.02647595,\n        -0.05486389,  0.001196  ],\n       [ 0.02087866,  0.03977818,  0.02973026, ..., -0.04849208,\n        -0.03428288, -0.03353313],\n       [-0.00754663,  0.00295051, -0.00115429, ..., -0.00839342,\n         0.01157734, -0.03211762],\n       ...,\n       [ 0.03940183, -0.02765681, -0.01243274, ...,  0.01878955,\n        -0.01772225,  0.05469738],\n       [-0.00503979,  0.01623228,  0.06277147, ..., -0.0290992 ,\n        -0.03663587,  0.01084558],\n       [-0.01480096,  0.0230887 , -0.04091615, ..., -0.04485734,\n        -0.05254643, -0.02245559]], dtype=float32)"
     },
     "metadata": {},
     "execution_count": 55
    }
   ],
   "source": [
    "word_embeddings = np.load('/Users/baeyuna/Documents/SNU_DLab/TANR_Bert/data/glove/word_embeddings.npy')\n",
    "word_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(44775, 768)"
     },
     "metadata": {},
     "execution_count": 56
    }
   ],
   "source": [
    "word_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                      0         1         2         3         4         5    \\\nwords                                                                         \nthe              0.022915  0.017409  0.010446 -0.048069  0.032508  0.013682   \nbrands           0.020879  0.039778  0.029730 -0.002456  0.059781 -0.025582   \nqueen           -0.007547  0.002951 -0.001154 -0.047338  0.037052  0.011151   \nelizabeth        0.019542 -0.045189  0.028748 -0.028304  0.034132  0.049069   \n,               -0.021864  0.011111  0.016383 -0.032495 -0.002032  0.032761   \n...                   ...       ...       ...       ...       ...       ...   \nkhizr            0.039653 -0.058133 -0.060750 -0.072139  0.026598  0.031439   \nedwards-helaire  0.040825 -0.037304  0.005907 -0.023890 -0.007193 -0.037433   \nkorg             0.039402 -0.027657 -0.012433 -0.051315  0.026456 -0.026098   \nminilogue       -0.005040  0.016232  0.062771 -0.027899 -0.003462 -0.002256   \n78-55           -0.014801  0.023089 -0.040916 -0.032936 -0.011149 -0.044840   \n\n                      6         7         8         9    ...       758  \\\nwords                                                    ...             \nthe             -0.043972  0.028114 -0.081688 -0.003506  ... -0.014207   \nbrands          -0.029728 -0.038067 -0.051656  0.040926  ... -0.004319   \nqueen           -0.070805 -0.029634 -0.012547 -0.008933  ...  0.030856   \nelizabeth       -0.076122  0.023633 -0.062191 -0.022393  ...  0.014009   \n,               -0.019221 -0.006823 -0.076860 -0.039014  ...  0.000077   \n...                   ...       ...       ...       ...  ...       ...   \nkhizr            0.009851  0.040807 -0.045499 -0.059950  ...  0.047280   \nedwards-helaire -0.071384 -0.007603 -0.018867  0.004944  ...  0.046952   \nkorg            -0.015726  0.053842 -0.047304 -0.019760  ...  0.005184   \nminilogue       -0.033712  0.025692  0.042241 -0.061289  ...  0.078346   \n78-55           -0.025739 -0.057561 -0.059291 -0.053538  ...  0.009649   \n\n                      759       760       761       762       763       764  \\\nwords                                                                         \nthe              0.003513 -0.056283  0.023020  0.047154  0.024799  0.025471   \nbrands           0.058174 -0.023548 -0.041602  0.001282  0.009522  0.026286   \nqueen            0.036562 -0.056421 -0.027629  0.028740  0.003800  0.027521   \nelizabeth        0.046649 -0.040938 -0.059904 -0.022907  0.016196 -0.041572   \n,                0.017377 -0.059505  0.064820  0.065166 -0.003043 -0.019843   \n...                   ...       ...       ...       ...       ...       ...   \nkhizr            0.013840 -0.034338  0.003750 -0.042403 -0.041079 -0.023069   \nedwards-helaire  0.050125  0.060447 -0.088661  0.059591  0.013882  0.007241   \nkorg             0.019469 -0.029712  0.017312 -0.042999 -0.055916  0.010879   \nminilogue        0.022015 -0.045793  0.005103  0.027438  0.003614  0.043436   \n78-55           -0.006906 -0.019531 -0.048854 -0.013579 -0.053662  0.025897   \n\n                      765       766       767  \nwords                                          \nthe              0.026476 -0.054864  0.001196  \nbrands          -0.048492 -0.034283 -0.033533  \nqueen           -0.008393  0.011577 -0.032118  \nelizabeth       -0.009329 -0.002110 -0.034657  \n,                0.010483 -0.066509 -0.006312  \n...                   ...       ...       ...  \nkhizr            0.008094 -0.011244  0.047785  \nedwards-helaire  0.041523 -0.038662  0.019831  \nkorg             0.018790 -0.017722  0.054697  \nminilogue       -0.029099 -0.036636  0.010846  \n78-55           -0.044857 -0.052546 -0.022456  \n\n[44775 rows x 768 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>758</th>\n      <th>759</th>\n      <th>760</th>\n      <th>761</th>\n      <th>762</th>\n      <th>763</th>\n      <th>764</th>\n      <th>765</th>\n      <th>766</th>\n      <th>767</th>\n    </tr>\n    <tr>\n      <th>words</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>the</th>\n      <td>0.022915</td>\n      <td>0.017409</td>\n      <td>0.010446</td>\n      <td>-0.048069</td>\n      <td>0.032508</td>\n      <td>0.013682</td>\n      <td>-0.043972</td>\n      <td>0.028114</td>\n      <td>-0.081688</td>\n      <td>-0.003506</td>\n      <td>...</td>\n      <td>-0.014207</td>\n      <td>0.003513</td>\n      <td>-0.056283</td>\n      <td>0.023020</td>\n      <td>0.047154</td>\n      <td>0.024799</td>\n      <td>0.025471</td>\n      <td>0.026476</td>\n      <td>-0.054864</td>\n      <td>0.001196</td>\n    </tr>\n    <tr>\n      <th>brands</th>\n      <td>0.020879</td>\n      <td>0.039778</td>\n      <td>0.029730</td>\n      <td>-0.002456</td>\n      <td>0.059781</td>\n      <td>-0.025582</td>\n      <td>-0.029728</td>\n      <td>-0.038067</td>\n      <td>-0.051656</td>\n      <td>0.040926</td>\n      <td>...</td>\n      <td>-0.004319</td>\n      <td>0.058174</td>\n      <td>-0.023548</td>\n      <td>-0.041602</td>\n      <td>0.001282</td>\n      <td>0.009522</td>\n      <td>0.026286</td>\n      <td>-0.048492</td>\n      <td>-0.034283</td>\n      <td>-0.033533</td>\n    </tr>\n    <tr>\n      <th>queen</th>\n      <td>-0.007547</td>\n      <td>0.002951</td>\n      <td>-0.001154</td>\n      <td>-0.047338</td>\n      <td>0.037052</td>\n      <td>0.011151</td>\n      <td>-0.070805</td>\n      <td>-0.029634</td>\n      <td>-0.012547</td>\n      <td>-0.008933</td>\n      <td>...</td>\n      <td>0.030856</td>\n      <td>0.036562</td>\n      <td>-0.056421</td>\n      <td>-0.027629</td>\n      <td>0.028740</td>\n      <td>0.003800</td>\n      <td>0.027521</td>\n      <td>-0.008393</td>\n      <td>0.011577</td>\n      <td>-0.032118</td>\n    </tr>\n    <tr>\n      <th>elizabeth</th>\n      <td>0.019542</td>\n      <td>-0.045189</td>\n      <td>0.028748</td>\n      <td>-0.028304</td>\n      <td>0.034132</td>\n      <td>0.049069</td>\n      <td>-0.076122</td>\n      <td>0.023633</td>\n      <td>-0.062191</td>\n      <td>-0.022393</td>\n      <td>...</td>\n      <td>0.014009</td>\n      <td>0.046649</td>\n      <td>-0.040938</td>\n      <td>-0.059904</td>\n      <td>-0.022907</td>\n      <td>0.016196</td>\n      <td>-0.041572</td>\n      <td>-0.009329</td>\n      <td>-0.002110</td>\n      <td>-0.034657</td>\n    </tr>\n    <tr>\n      <th>,</th>\n      <td>-0.021864</td>\n      <td>0.011111</td>\n      <td>0.016383</td>\n      <td>-0.032495</td>\n      <td>-0.002032</td>\n      <td>0.032761</td>\n      <td>-0.019221</td>\n      <td>-0.006823</td>\n      <td>-0.076860</td>\n      <td>-0.039014</td>\n      <td>...</td>\n      <td>0.000077</td>\n      <td>0.017377</td>\n      <td>-0.059505</td>\n      <td>0.064820</td>\n      <td>0.065166</td>\n      <td>-0.003043</td>\n      <td>-0.019843</td>\n      <td>0.010483</td>\n      <td>-0.066509</td>\n      <td>-0.006312</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>khizr</th>\n      <td>0.039653</td>\n      <td>-0.058133</td>\n      <td>-0.060750</td>\n      <td>-0.072139</td>\n      <td>0.026598</td>\n      <td>0.031439</td>\n      <td>0.009851</td>\n      <td>0.040807</td>\n      <td>-0.045499</td>\n      <td>-0.059950</td>\n      <td>...</td>\n      <td>0.047280</td>\n      <td>0.013840</td>\n      <td>-0.034338</td>\n      <td>0.003750</td>\n      <td>-0.042403</td>\n      <td>-0.041079</td>\n      <td>-0.023069</td>\n      <td>0.008094</td>\n      <td>-0.011244</td>\n      <td>0.047785</td>\n    </tr>\n    <tr>\n      <th>edwards-helaire</th>\n      <td>0.040825</td>\n      <td>-0.037304</td>\n      <td>0.005907</td>\n      <td>-0.023890</td>\n      <td>-0.007193</td>\n      <td>-0.037433</td>\n      <td>-0.071384</td>\n      <td>-0.007603</td>\n      <td>-0.018867</td>\n      <td>0.004944</td>\n      <td>...</td>\n      <td>0.046952</td>\n      <td>0.050125</td>\n      <td>0.060447</td>\n      <td>-0.088661</td>\n      <td>0.059591</td>\n      <td>0.013882</td>\n      <td>0.007241</td>\n      <td>0.041523</td>\n      <td>-0.038662</td>\n      <td>0.019831</td>\n    </tr>\n    <tr>\n      <th>korg</th>\n      <td>0.039402</td>\n      <td>-0.027657</td>\n      <td>-0.012433</td>\n      <td>-0.051315</td>\n      <td>0.026456</td>\n      <td>-0.026098</td>\n      <td>-0.015726</td>\n      <td>0.053842</td>\n      <td>-0.047304</td>\n      <td>-0.019760</td>\n      <td>...</td>\n      <td>0.005184</td>\n      <td>0.019469</td>\n      <td>-0.029712</td>\n      <td>0.017312</td>\n      <td>-0.042999</td>\n      <td>-0.055916</td>\n      <td>0.010879</td>\n      <td>0.018790</td>\n      <td>-0.017722</td>\n      <td>0.054697</td>\n    </tr>\n    <tr>\n      <th>minilogue</th>\n      <td>-0.005040</td>\n      <td>0.016232</td>\n      <td>0.062771</td>\n      <td>-0.027899</td>\n      <td>-0.003462</td>\n      <td>-0.002256</td>\n      <td>-0.033712</td>\n      <td>0.025692</td>\n      <td>0.042241</td>\n      <td>-0.061289</td>\n      <td>...</td>\n      <td>0.078346</td>\n      <td>0.022015</td>\n      <td>-0.045793</td>\n      <td>0.005103</td>\n      <td>0.027438</td>\n      <td>0.003614</td>\n      <td>0.043436</td>\n      <td>-0.029099</td>\n      <td>-0.036636</td>\n      <td>0.010846</td>\n    </tr>\n    <tr>\n      <th>78-55</th>\n      <td>-0.014801</td>\n      <td>0.023089</td>\n      <td>-0.040916</td>\n      <td>-0.032936</td>\n      <td>-0.011149</td>\n      <td>-0.044840</td>\n      <td>-0.025739</td>\n      <td>-0.057561</td>\n      <td>-0.059291</td>\n      <td>-0.053538</td>\n      <td>...</td>\n      <td>0.009649</td>\n      <td>-0.006906</td>\n      <td>-0.019531</td>\n      <td>-0.048854</td>\n      <td>-0.013579</td>\n      <td>-0.053662</td>\n      <td>0.025897</td>\n      <td>-0.044857</td>\n      <td>-0.052546</td>\n      <td>-0.022456</td>\n    </tr>\n  </tbody>\n</table>\n<p>44775 rows × 768 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 62
    }
   ],
   "source": [
    "df = pd.DataFrame(word_embeddings)\n",
    "df.loc[:,'words'] = word2int.index.tolist()\n",
    "df = df.set_index('words')\n",
    "source_embedding = df.copy()\n",
    "source_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "            0         1         2         3         4         5         6    \\\nint                                                                           \n1      0.022915  0.017409  0.010446 -0.048069  0.032508  0.013682 -0.043972   \n2      0.020879  0.039778  0.029730 -0.002456  0.059781 -0.025582 -0.029728   \n3     -0.007547  0.002951 -0.001154 -0.047338  0.037052  0.011151 -0.070805   \n4      0.019542 -0.045189  0.028748 -0.028304  0.034132  0.049069 -0.076122   \n5     -0.021864  0.011111  0.016383 -0.032495 -0.002032  0.032761 -0.019221   \n...         ...       ...       ...       ...       ...       ...       ...   \n44771  0.039653 -0.058133 -0.060750 -0.072139  0.026598  0.031439  0.009851   \n44772  0.040825 -0.037304  0.005907 -0.023890 -0.007193 -0.037433 -0.071384   \n44773  0.039402 -0.027657 -0.012433 -0.051315  0.026456 -0.026098 -0.015726   \n44774 -0.005040  0.016232  0.062771 -0.027899 -0.003462 -0.002256 -0.033712   \n44775 -0.014801  0.023089 -0.040916 -0.032936 -0.011149 -0.044840 -0.025739   \n\n            7         8         9    ...       758       759       760  \\\nint                                  ...                                 \n1      0.028114 -0.081688 -0.003506  ... -0.014207  0.003513 -0.056283   \n2     -0.038067 -0.051656  0.040926  ... -0.004319  0.058174 -0.023548   \n3     -0.029634 -0.012547 -0.008933  ...  0.030856  0.036562 -0.056421   \n4      0.023633 -0.062191 -0.022393  ...  0.014009  0.046649 -0.040938   \n5     -0.006823 -0.076860 -0.039014  ...  0.000077  0.017377 -0.059505   \n...         ...       ...       ...  ...       ...       ...       ...   \n44771  0.040807 -0.045499 -0.059950  ...  0.047280  0.013840 -0.034338   \n44772 -0.007603 -0.018867  0.004944  ...  0.046952  0.050125  0.060447   \n44773  0.053842 -0.047304 -0.019760  ...  0.005184  0.019469 -0.029712   \n44774  0.025692  0.042241 -0.061289  ...  0.078346  0.022015 -0.045793   \n44775 -0.057561 -0.059291 -0.053538  ...  0.009649 -0.006906 -0.019531   \n\n            761       762       763       764       765       766       767  \nint                                                                          \n1      0.023020  0.047154  0.024799  0.025471  0.026476 -0.054864  0.001196  \n2     -0.041602  0.001282  0.009522  0.026286 -0.048492 -0.034283 -0.033533  \n3     -0.027629  0.028740  0.003800  0.027521 -0.008393  0.011577 -0.032118  \n4     -0.059904 -0.022907  0.016196 -0.041572 -0.009329 -0.002110 -0.034657  \n5      0.064820  0.065166 -0.003043 -0.019843  0.010483 -0.066509 -0.006312  \n...         ...       ...       ...       ...       ...       ...       ...  \n44771  0.003750 -0.042403 -0.041079 -0.023069  0.008094 -0.011244  0.047785  \n44772 -0.088661  0.059591  0.013882  0.007241  0.041523 -0.038662  0.019831  \n44773  0.017312 -0.042999 -0.055916  0.010879  0.018790 -0.017722  0.054697  \n44774  0.005103  0.027438  0.003614  0.043436 -0.029099 -0.036636  0.010846  \n44775 -0.048854 -0.013579 -0.053662  0.025897 -0.044857 -0.052546 -0.022456  \n\n[44775 rows x 768 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>758</th>\n      <th>759</th>\n      <th>760</th>\n      <th>761</th>\n      <th>762</th>\n      <th>763</th>\n      <th>764</th>\n      <th>765</th>\n      <th>766</th>\n      <th>767</th>\n    </tr>\n    <tr>\n      <th>int</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>0.022915</td>\n      <td>0.017409</td>\n      <td>0.010446</td>\n      <td>-0.048069</td>\n      <td>0.032508</td>\n      <td>0.013682</td>\n      <td>-0.043972</td>\n      <td>0.028114</td>\n      <td>-0.081688</td>\n      <td>-0.003506</td>\n      <td>...</td>\n      <td>-0.014207</td>\n      <td>0.003513</td>\n      <td>-0.056283</td>\n      <td>0.023020</td>\n      <td>0.047154</td>\n      <td>0.024799</td>\n      <td>0.025471</td>\n      <td>0.026476</td>\n      <td>-0.054864</td>\n      <td>0.001196</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.020879</td>\n      <td>0.039778</td>\n      <td>0.029730</td>\n      <td>-0.002456</td>\n      <td>0.059781</td>\n      <td>-0.025582</td>\n      <td>-0.029728</td>\n      <td>-0.038067</td>\n      <td>-0.051656</td>\n      <td>0.040926</td>\n      <td>...</td>\n      <td>-0.004319</td>\n      <td>0.058174</td>\n      <td>-0.023548</td>\n      <td>-0.041602</td>\n      <td>0.001282</td>\n      <td>0.009522</td>\n      <td>0.026286</td>\n      <td>-0.048492</td>\n      <td>-0.034283</td>\n      <td>-0.033533</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-0.007547</td>\n      <td>0.002951</td>\n      <td>-0.001154</td>\n      <td>-0.047338</td>\n      <td>0.037052</td>\n      <td>0.011151</td>\n      <td>-0.070805</td>\n      <td>-0.029634</td>\n      <td>-0.012547</td>\n      <td>-0.008933</td>\n      <td>...</td>\n      <td>0.030856</td>\n      <td>0.036562</td>\n      <td>-0.056421</td>\n      <td>-0.027629</td>\n      <td>0.028740</td>\n      <td>0.003800</td>\n      <td>0.027521</td>\n      <td>-0.008393</td>\n      <td>0.011577</td>\n      <td>-0.032118</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.019542</td>\n      <td>-0.045189</td>\n      <td>0.028748</td>\n      <td>-0.028304</td>\n      <td>0.034132</td>\n      <td>0.049069</td>\n      <td>-0.076122</td>\n      <td>0.023633</td>\n      <td>-0.062191</td>\n      <td>-0.022393</td>\n      <td>...</td>\n      <td>0.014009</td>\n      <td>0.046649</td>\n      <td>-0.040938</td>\n      <td>-0.059904</td>\n      <td>-0.022907</td>\n      <td>0.016196</td>\n      <td>-0.041572</td>\n      <td>-0.009329</td>\n      <td>-0.002110</td>\n      <td>-0.034657</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>-0.021864</td>\n      <td>0.011111</td>\n      <td>0.016383</td>\n      <td>-0.032495</td>\n      <td>-0.002032</td>\n      <td>0.032761</td>\n      <td>-0.019221</td>\n      <td>-0.006823</td>\n      <td>-0.076860</td>\n      <td>-0.039014</td>\n      <td>...</td>\n      <td>0.000077</td>\n      <td>0.017377</td>\n      <td>-0.059505</td>\n      <td>0.064820</td>\n      <td>0.065166</td>\n      <td>-0.003043</td>\n      <td>-0.019843</td>\n      <td>0.010483</td>\n      <td>-0.066509</td>\n      <td>-0.006312</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>44771</th>\n      <td>0.039653</td>\n      <td>-0.058133</td>\n      <td>-0.060750</td>\n      <td>-0.072139</td>\n      <td>0.026598</td>\n      <td>0.031439</td>\n      <td>0.009851</td>\n      <td>0.040807</td>\n      <td>-0.045499</td>\n      <td>-0.059950</td>\n      <td>...</td>\n      <td>0.047280</td>\n      <td>0.013840</td>\n      <td>-0.034338</td>\n      <td>0.003750</td>\n      <td>-0.042403</td>\n      <td>-0.041079</td>\n      <td>-0.023069</td>\n      <td>0.008094</td>\n      <td>-0.011244</td>\n      <td>0.047785</td>\n    </tr>\n    <tr>\n      <th>44772</th>\n      <td>0.040825</td>\n      <td>-0.037304</td>\n      <td>0.005907</td>\n      <td>-0.023890</td>\n      <td>-0.007193</td>\n      <td>-0.037433</td>\n      <td>-0.071384</td>\n      <td>-0.007603</td>\n      <td>-0.018867</td>\n      <td>0.004944</td>\n      <td>...</td>\n      <td>0.046952</td>\n      <td>0.050125</td>\n      <td>0.060447</td>\n      <td>-0.088661</td>\n      <td>0.059591</td>\n      <td>0.013882</td>\n      <td>0.007241</td>\n      <td>0.041523</td>\n      <td>-0.038662</td>\n      <td>0.019831</td>\n    </tr>\n    <tr>\n      <th>44773</th>\n      <td>0.039402</td>\n      <td>-0.027657</td>\n      <td>-0.012433</td>\n      <td>-0.051315</td>\n      <td>0.026456</td>\n      <td>-0.026098</td>\n      <td>-0.015726</td>\n      <td>0.053842</td>\n      <td>-0.047304</td>\n      <td>-0.019760</td>\n      <td>...</td>\n      <td>0.005184</td>\n      <td>0.019469</td>\n      <td>-0.029712</td>\n      <td>0.017312</td>\n      <td>-0.042999</td>\n      <td>-0.055916</td>\n      <td>0.010879</td>\n      <td>0.018790</td>\n      <td>-0.017722</td>\n      <td>0.054697</td>\n    </tr>\n    <tr>\n      <th>44774</th>\n      <td>-0.005040</td>\n      <td>0.016232</td>\n      <td>0.062771</td>\n      <td>-0.027899</td>\n      <td>-0.003462</td>\n      <td>-0.002256</td>\n      <td>-0.033712</td>\n      <td>0.025692</td>\n      <td>0.042241</td>\n      <td>-0.061289</td>\n      <td>...</td>\n      <td>0.078346</td>\n      <td>0.022015</td>\n      <td>-0.045793</td>\n      <td>0.005103</td>\n      <td>0.027438</td>\n      <td>0.003614</td>\n      <td>0.043436</td>\n      <td>-0.029099</td>\n      <td>-0.036636</td>\n      <td>0.010846</td>\n    </tr>\n    <tr>\n      <th>44775</th>\n      <td>-0.014801</td>\n      <td>0.023089</td>\n      <td>-0.040916</td>\n      <td>-0.032936</td>\n      <td>-0.011149</td>\n      <td>-0.044840</td>\n      <td>-0.025739</td>\n      <td>-0.057561</td>\n      <td>-0.059291</td>\n      <td>-0.053538</td>\n      <td>...</td>\n      <td>0.009649</td>\n      <td>-0.006906</td>\n      <td>-0.019531</td>\n      <td>-0.048854</td>\n      <td>-0.013579</td>\n      <td>-0.053662</td>\n      <td>0.025897</td>\n      <td>-0.044857</td>\n      <td>-0.052546</td>\n      <td>-0.022456</td>\n    </tr>\n  </tbody>\n</table>\n<p>44775 rows × 768 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 63
    }
   ],
   "source": [
    "# merged = word2int.merge(source_embedding,\n",
    "#                             how='inner',\n",
    "#                             left_index=True,\n",
    "#                             right_index=True)\n",
    "# merged.set_index('int', inplace=True)\n",
    "# merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "          0        1         2         3         4         5         6    \\\nint                                                                        \n0   -0.737502 -1.90175 -0.014587  1.887085  2.155873 -0.248314  0.015405   \n\n          7         8         9    ...       758      759       760       761  \\\nint                                ...                                          \n0   -1.485848  0.121748  1.092142  ...  0.526911 -1.82094  0.622896 -0.093258   \n\n          762       763       764       765      766      767  \nint                                                            \n0   -0.343962 -0.602187  0.827893  1.403123 -0.80017  0.17592  \n\n[1 rows x 768 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>758</th>\n      <th>759</th>\n      <th>760</th>\n      <th>761</th>\n      <th>762</th>\n      <th>763</th>\n      <th>764</th>\n      <th>765</th>\n      <th>766</th>\n      <th>767</th>\n    </tr>\n    <tr>\n      <th>int</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-0.737502</td>\n      <td>-1.90175</td>\n      <td>-0.014587</td>\n      <td>1.887085</td>\n      <td>2.155873</td>\n      <td>-0.248314</td>\n      <td>0.015405</td>\n      <td>-1.485848</td>\n      <td>0.121748</td>\n      <td>1.092142</td>\n      <td>...</td>\n      <td>0.526911</td>\n      <td>-1.82094</td>\n      <td>0.622896</td>\n      <td>-0.093258</td>\n      <td>-0.343962</td>\n      <td>-0.602187</td>\n      <td>0.827893</td>\n      <td>1.403123</td>\n      <td>-0.80017</td>\n      <td>0.17592</td>\n    </tr>\n  </tbody>\n</table>\n<p>1 rows × 768 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 64
    }
   ],
   "source": [
    "# missed_index = np.setdiff1d(np.arange(len(word2int) + 1),\n",
    "#                             merged.index.values)\n",
    "# missed_embedding = pd.DataFrame(data=np.random.normal(\n",
    "#     size=(len(missed_index), 768)))\n",
    "# missed_embedding['int'] = missed_index\n",
    "# missed_embedding.set_index('int', inplace=True)\n",
    "# missed_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_embedding = pd.concat([merged, missed_embedding]).sort_index()\n",
    "# final_embedding\n",
    "np.save('pretrained_word_embedding.npy', merged.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[-0.78346185,  0.39302635, -0.71049564, ...,  1.1914767 ,\n        -0.58732314, -1.08972927],\n       [ 0.27204   , -0.06203   , -0.1884    , ...,  0.13015   ,\n        -0.18317   ,  0.1323    ],\n       [ 0.30791   ,  0.12428   , -0.045487  , ..., -0.4962    ,\n         0.56847   ,  0.51508   ],\n       ...,\n       [ 0.041715  ,  0.85602   ,  0.031382  , ...,  0.75407   ,\n        -0.72133   ,  0.19725   ],\n       [-0.24537994,  0.24460628, -0.9640435 , ...,  0.70863113,\n         0.23182131,  0.37770902],\n       [-0.044587  , -0.37936   ,  0.40889   , ...,  0.8848    ,\n        -0.41805   , -0.21511   ]])"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "pretrained_word_embedding = np.load('/Users/baeyuna/Documents/SNU_DLab/NAML/data/train/pretrained_word_embedding.npy')\n",
    "pretrained_word_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "44775"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "word_lst = word2int.index.tolist()\n",
    "len(word_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bert WordEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "def get_model(model_url, max_seq_length):\n",
    "  labse_layer = hub.KerasLayer(model_url, trainable=True)\n",
    "\n",
    "  # Define input.\n",
    "  input_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
    "                                         name=\"input_word_ids\")\n",
    "  input_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
    "                                     name=\"input_mask\")\n",
    "  segment_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
    "                                      name=\"segment_ids\")\n",
    "\n",
    "  # LaBSE layer.\n",
    "  pooled_output,  _ = labse_layer([input_word_ids, input_mask, segment_ids])\n",
    "\n",
    "  # The embedding is l2 normalized.\n",
    "  pooled_output = tf.keras.layers.Lambda(\n",
    "      lambda x: tf.nn.l2_normalize(x, axis=1))(pooled_output)\n",
    "\n",
    "  # Define model.\n",
    "  return tf.keras.Model(\n",
    "        inputs=[input_word_ids, input_mask, segment_ids],\n",
    "        outputs=pooled_output), labse_layer\n",
    "\n",
    "max_seq_length = 64\n",
    "labse_model, labse_layer = get_model(\n",
    "    model_url=\"https://tfhub.dev/google/LaBSE/1\", max_seq_length=max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bert\n",
    "\n",
    "vocab_file = labse_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "do_lower_case = labse_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = bert.bert_tokenization.FullTokenizer(vocab_file, do_lower_case)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_input(input_strings, tokenizer, max_seq_length):\n",
    "\n",
    "  input_ids_all, input_mask_all, segment_ids_all = [], [], []\n",
    "  for input_string in input_strings:\n",
    "    # Tokenize input.\n",
    "    input_tokens = [\"[CLS]\"] + tokenizer.tokenize(input_string) + [\"[SEP]\"]\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(input_tokens)\n",
    "    sequence_length = min(len(input_ids), max_seq_length)\n",
    "\n",
    "    # Padding or truncation.\n",
    "    if len(input_ids) >= max_seq_length:\n",
    "      input_ids = input_ids[:max_seq_length]\n",
    "    else:\n",
    "      input_ids = input_ids + [0] * (max_seq_length - len(input_ids))\n",
    "\n",
    "    input_mask = [1] * sequence_length + [0] * (max_seq_length - sequence_length)\n",
    "\n",
    "    input_ids_all.append(input_ids)\n",
    "    input_mask_all.append(input_mask)\n",
    "    segment_ids_all.append([0] * max_seq_length)\n",
    "\n",
    "  return np.array(input_ids_all), np.array(input_mask_all), np.array(segment_ids_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(input_text):\n",
    "  input_ids, input_mask, segment_ids = create_input(\n",
    "    input_text, tokenizer, max_seq_length)\n",
    "  return labse_model([input_ids, input_mask, segment_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tf.Tensor: shape=(3, 768), dtype=float32, numpy=\narray([[-0.01026385, -0.00157138, -0.01473237, ...,  0.0494036 ,\n        -0.01296441, -0.03172865],\n       [-0.05280595, -0.00894449, -0.05583495, ..., -0.00754025,\n         0.01059245, -0.01610355],\n       [ 0.02716724, -0.00592896,  0.01995578, ..., -0.00215263,\n         0.00364844, -0.03194168]], dtype=float32)>"
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "english_sentences = [\"dog\", \"Puppies are nice.\", \"I enjoy taking long walks along the beach with my dog.\"]\n",
    "\n",
    "english_embeddings = encode(english_sentences)\n",
    "english_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('x_save', english_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "word_lst_embeddings1 = np.load('/Users/baeyuna/Documents/SNU_DLab/TANR_Bert/data/glove/word_lst_embeddings1.npy')\n",
    "word_lst_embeddings10 = np.load('/Users/baeyuna/Documents/SNU_DLab/TANR_Bert/data/glove/word_lst_embeddings10.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(2000, 768)"
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "word_lst_embeddings10.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(2000, 768)"
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "word_lst_embeddings1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(4000, 768)"
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "source": [
    "np.concatenate([word_lst_embeddings1, word_lst_embeddings10]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(2000, 768)"
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "word_lst_embeddings1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[ 0.02291545,  0.01740933,  0.01044573, ...,  0.02647595,\n        -0.05486389,  0.001196  ],\n       [ 0.02087866,  0.03977818,  0.02973026, ..., -0.04849208,\n        -0.03428288, -0.03353313],\n       [-0.00754663,  0.00295051, -0.00115429, ..., -0.00839342,\n         0.01157734, -0.03211762],\n       ...,\n       [ 0.01743446, -0.05396994, -0.0726243 , ...,  0.00198761,\n        -0.03869714,  0.01757814],\n       [ 0.01157075,  0.03626885, -0.01124088, ...,  0.01655105,\n        -0.03771622, -0.04456491],\n       [-0.02233236, -0.0285336 , -0.02215027, ...,  0.01747822,\n        -0.04677428, -0.01103436]], dtype=float32)"
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "word_lst_embeddings1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "44775"
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "len(word_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "22"
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "44775//2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tf.Tensor: shape=(3, 768), dtype=float32, numpy=\narray([[-0.01112244,  0.00602975, -0.00272447, ...,  0.0336603 ,\n        -0.02208618, -0.05659664],\n       [-0.04087604, -0.01662398, -0.04726062, ...,  0.03640377,\n         0.02879217, -0.03352933],\n       [-0.01007188, -0.01358392, -0.03919495, ...,  0.0337332 ,\n        -0.01138729, -0.05158317]], dtype=float32)>"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "word_lst_embeddings = encode(word_lst)\n",
    "word_lst_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('D:/admin/Documents/x_save', word_lst_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "         0           1           2            3           4          5    \\\n0 -0.0111224  0.00602975 -0.00272447  0.000836123  -0.0263209  0.0247632   \n1  -0.040876   -0.016624  -0.0472606   -0.0340528  -0.0378542  0.0239878   \n2 -0.0100719  -0.0135839   -0.039195   -0.0589811  0.00538327    0.02524   \n\n         6           7          8          9    ...        758         759  \\\n0  0.0511221   0.0262025 -0.0621353 -0.0558861  ...  0.0116968 -0.00990636   \n1  0.0426077  0.00444569 -0.0612404 -0.0501724  ...  0.0101625  -0.0207745   \n2  0.0306217  -0.0189067 -0.0431255  0.0504395  ... -0.0536857  -0.0311351   \n\n          760        761         762         763        764        765  \\\n0   0.0337292  -0.014268  -0.0140283 -0.00477358 -0.0121225  0.0336603   \n1   0.0282323 -0.0286058 -0.00483788 -0.00685988  0.0179411  0.0364038   \n2  0.00727072  -0.038046   -0.023034   -0.015535 -0.0318728  0.0337332   \n\n         766        767  \n0 -0.0220862 -0.0565966  \n1  0.0287922 -0.0335293  \n2 -0.0113873 -0.0515832  \n\n[3 rows x 768 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>758</th>\n      <th>759</th>\n      <th>760</th>\n      <th>761</th>\n      <th>762</th>\n      <th>763</th>\n      <th>764</th>\n      <th>765</th>\n      <th>766</th>\n      <th>767</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-0.0111224</td>\n      <td>0.00602975</td>\n      <td>-0.00272447</td>\n      <td>0.000836123</td>\n      <td>-0.0263209</td>\n      <td>0.0247632</td>\n      <td>0.0511221</td>\n      <td>0.0262025</td>\n      <td>-0.0621353</td>\n      <td>-0.0558861</td>\n      <td>...</td>\n      <td>0.0116968</td>\n      <td>-0.00990636</td>\n      <td>0.0337292</td>\n      <td>-0.014268</td>\n      <td>-0.0140283</td>\n      <td>-0.00477358</td>\n      <td>-0.0121225</td>\n      <td>0.0336603</td>\n      <td>-0.0220862</td>\n      <td>-0.0565966</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-0.040876</td>\n      <td>-0.016624</td>\n      <td>-0.0472606</td>\n      <td>-0.0340528</td>\n      <td>-0.0378542</td>\n      <td>0.0239878</td>\n      <td>0.0426077</td>\n      <td>0.00444569</td>\n      <td>-0.0612404</td>\n      <td>-0.0501724</td>\n      <td>...</td>\n      <td>0.0101625</td>\n      <td>-0.0207745</td>\n      <td>0.0282323</td>\n      <td>-0.0286058</td>\n      <td>-0.00483788</td>\n      <td>-0.00685988</td>\n      <td>0.0179411</td>\n      <td>0.0364038</td>\n      <td>0.0287922</td>\n      <td>-0.0335293</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-0.0100719</td>\n      <td>-0.0135839</td>\n      <td>-0.039195</td>\n      <td>-0.0589811</td>\n      <td>0.00538327</td>\n      <td>0.02524</td>\n      <td>0.0306217</td>\n      <td>-0.0189067</td>\n      <td>-0.0431255</td>\n      <td>0.0504395</td>\n      <td>...</td>\n      <td>-0.0536857</td>\n      <td>-0.0311351</td>\n      <td>0.00727072</td>\n      <td>-0.038046</td>\n      <td>-0.023034</td>\n      <td>-0.015535</td>\n      <td>-0.0318728</td>\n      <td>0.0337332</td>\n      <td>-0.0113873</td>\n      <td>-0.0515832</td>\n    </tr>\n  </tbody>\n</table>\n<p>3 rows × 768 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "df = pd.DataFrame(index=range(0,word_lst_embeddings.shape[0]), columns=range(0,word_lst_embeddings.shape[1]))\n",
    "\n",
    "for i in range(word_lst_embeddings.shape[0]):\n",
    "    df.iloc[i,:] = word_lst_embeddings[i]\n",
    "\n",
    "df.loc[:,'words'] = word_lst \n",
    "df = df.rename_axis('words').iloc[:,:-1]\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = word2int.merge(source_embedding,\n",
    "                            how='inner',\n",
    "                            left_index=True,\n",
    "                            right_index=True)\n",
    "merged.set_index('int', inplace=True)\n",
    "merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missed_index = np.setdiff1d(np.arange(len(word2int) + 1),\n",
    "                            merged.index.values)\n",
    "missed_embedding = pd.DataFrame(data=np.random.normal(\n",
    "    size=(len(missed_index), 300)))\n",
    "missed_embedding['int'] = missed_index\n",
    "missed_embedding.set_index('int', inplace=True)\n",
    "missed_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_embedding = pd.concat([merged, missed_embedding]).sort_index()\n",
    "np.save('pretrained_word_embedding.npy', final_embedding.values)\n",
    "\n",
    "print(\n",
    "    f'Rate of word missed in pretrained embedding: {(len(missed_index)-1)/len(word2int):.4f}'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "mind",
   "display_name": "mind"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}