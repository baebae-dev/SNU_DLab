{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import model_name\n",
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from os import path\n",
    "import random\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import csv\n",
    "from pathlib import Path\n",
    "from shutil import copyfile\n",
    "import importlib\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "          0         1        2         3         4         5         6    \\\n,   -0.082752  0.672040 -0.14987 -0.064983  0.056491  0.402280  0.002775   \n.    0.012001  0.207510 -0.12578 -0.593250  0.125250  0.159750  0.137480   \nthe  0.272040 -0.062030 -0.18840  0.023225 -0.018158  0.006719 -0.138770   \nand -0.185670  0.066008 -0.25209 -0.117250  0.265130  0.064908  0.122910   \nto   0.319240  0.063160 -0.27858  0.261200  0.079248 -0.214620 -0.104950   \n\n          7         8       9    ...      290       291      292       293  \\\n,   -0.331100 -0.306910  2.0817  ... -0.14331  0.018267 -0.18643  0.207090   \n.   -0.331570 -0.136940  1.7893  ...  0.16165 -0.066737 -0.29556  0.022612   \nthe  0.177080  0.177090  2.5882  ... -0.42810  0.168990  0.22511 -0.285570   \nand -0.093979  0.024321  2.4926  ... -0.59396 -0.097729  0.20072  0.170550   \nto   0.154950 -0.033530  2.4834  ... -0.12977  0.371300  0.18888 -0.004274   \n\n          294       295       296       297       298      299  \n,   -0.355980  0.053380 -0.050821 -0.191800 -0.378460 -0.06589  \n.   -0.281350  0.063500  0.140190  0.138710 -0.360490 -0.03500  \nthe -0.102800 -0.018168  0.114070  0.130150 -0.183170  0.13230  \nand -0.004736 -0.039709  0.324980 -0.023452  0.123020  0.33120  \nto  -0.106450 -0.258100 -0.044629  0.082745  0.097801  0.25045  \n\n[5 rows x 300 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>290</th>\n      <th>291</th>\n      <th>292</th>\n      <th>293</th>\n      <th>294</th>\n      <th>295</th>\n      <th>296</th>\n      <th>297</th>\n      <th>298</th>\n      <th>299</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>,</th>\n      <td>-0.082752</td>\n      <td>0.672040</td>\n      <td>-0.14987</td>\n      <td>-0.064983</td>\n      <td>0.056491</td>\n      <td>0.402280</td>\n      <td>0.002775</td>\n      <td>-0.331100</td>\n      <td>-0.306910</td>\n      <td>2.0817</td>\n      <td>...</td>\n      <td>-0.14331</td>\n      <td>0.018267</td>\n      <td>-0.18643</td>\n      <td>0.207090</td>\n      <td>-0.355980</td>\n      <td>0.053380</td>\n      <td>-0.050821</td>\n      <td>-0.191800</td>\n      <td>-0.378460</td>\n      <td>-0.06589</td>\n    </tr>\n    <tr>\n      <th>.</th>\n      <td>0.012001</td>\n      <td>0.207510</td>\n      <td>-0.12578</td>\n      <td>-0.593250</td>\n      <td>0.125250</td>\n      <td>0.159750</td>\n      <td>0.137480</td>\n      <td>-0.331570</td>\n      <td>-0.136940</td>\n      <td>1.7893</td>\n      <td>...</td>\n      <td>0.16165</td>\n      <td>-0.066737</td>\n      <td>-0.29556</td>\n      <td>0.022612</td>\n      <td>-0.281350</td>\n      <td>0.063500</td>\n      <td>0.140190</td>\n      <td>0.138710</td>\n      <td>-0.360490</td>\n      <td>-0.03500</td>\n    </tr>\n    <tr>\n      <th>the</th>\n      <td>0.272040</td>\n      <td>-0.062030</td>\n      <td>-0.18840</td>\n      <td>0.023225</td>\n      <td>-0.018158</td>\n      <td>0.006719</td>\n      <td>-0.138770</td>\n      <td>0.177080</td>\n      <td>0.177090</td>\n      <td>2.5882</td>\n      <td>...</td>\n      <td>-0.42810</td>\n      <td>0.168990</td>\n      <td>0.22511</td>\n      <td>-0.285570</td>\n      <td>-0.102800</td>\n      <td>-0.018168</td>\n      <td>0.114070</td>\n      <td>0.130150</td>\n      <td>-0.183170</td>\n      <td>0.13230</td>\n    </tr>\n    <tr>\n      <th>and</th>\n      <td>-0.185670</td>\n      <td>0.066008</td>\n      <td>-0.25209</td>\n      <td>-0.117250</td>\n      <td>0.265130</td>\n      <td>0.064908</td>\n      <td>0.122910</td>\n      <td>-0.093979</td>\n      <td>0.024321</td>\n      <td>2.4926</td>\n      <td>...</td>\n      <td>-0.59396</td>\n      <td>-0.097729</td>\n      <td>0.20072</td>\n      <td>0.170550</td>\n      <td>-0.004736</td>\n      <td>-0.039709</td>\n      <td>0.324980</td>\n      <td>-0.023452</td>\n      <td>0.123020</td>\n      <td>0.33120</td>\n    </tr>\n    <tr>\n      <th>to</th>\n      <td>0.319240</td>\n      <td>0.063160</td>\n      <td>-0.27858</td>\n      <td>0.261200</td>\n      <td>0.079248</td>\n      <td>-0.214620</td>\n      <td>-0.104950</td>\n      <td>0.154950</td>\n      <td>-0.033530</td>\n      <td>2.4834</td>\n      <td>...</td>\n      <td>-0.12977</td>\n      <td>0.371300</td>\n      <td>0.18888</td>\n      <td>-0.004274</td>\n      <td>-0.106450</td>\n      <td>-0.258100</td>\n      <td>-0.044629</td>\n      <td>0.082745</td>\n      <td>0.097801</td>\n      <td>0.25045</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 300 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "source_embedding = pd.read_table('/Users/baeyuna/Documents/SNU_DLab/TANR_Bert/data/glove/glove.840B.300d.txt',\n",
    "                                     index_col=0,\n",
    "                                     sep=' ',\n",
    "                                     header=None,\n",
    "                                     quoting=csv.QUOTE_NONE,\n",
    "                                     names=range(300))\n",
    "source_embedding.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "           0         1        2         3         4         5         6    \\\nword                                                                        \n,    -0.082752  0.672040 -0.14987 -0.064983  0.056491  0.402280  0.002775   \n.     0.012001  0.207510 -0.12578 -0.593250  0.125250  0.159750  0.137480   \nthe   0.272040 -0.062030 -0.18840  0.023225 -0.018158  0.006719 -0.138770   \nand  -0.185670  0.066008 -0.25209 -0.117250  0.265130  0.064908  0.122910   \nto    0.319240  0.063160 -0.27858  0.261200  0.079248 -0.214620 -0.104950   \n\n           7         8       9    ...      290       291      292       293  \\\nword                              ...                                         \n,    -0.331100 -0.306910  2.0817  ... -0.14331  0.018267 -0.18643  0.207090   \n.    -0.331570 -0.136940  1.7893  ...  0.16165 -0.066737 -0.29556  0.022612   \nthe   0.177080  0.177090  2.5882  ... -0.42810  0.168990  0.22511 -0.285570   \nand  -0.093979  0.024321  2.4926  ... -0.59396 -0.097729  0.20072  0.170550   \nto    0.154950 -0.033530  2.4834  ... -0.12977  0.371300  0.18888 -0.004274   \n\n           294       295       296       297       298      299  \nword                                                             \n,    -0.355980  0.053380 -0.050821 -0.191800 -0.378460 -0.06589  \n.    -0.281350  0.063500  0.140190  0.138710 -0.360490 -0.03500  \nthe  -0.102800 -0.018168  0.114070  0.130150 -0.183170  0.13230  \nand  -0.004736 -0.039709  0.324980 -0.023452  0.123020  0.33120  \nto   -0.106450 -0.258100 -0.044629  0.082745  0.097801  0.25045  \n\n[5 rows x 300 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>290</th>\n      <th>291</th>\n      <th>292</th>\n      <th>293</th>\n      <th>294</th>\n      <th>295</th>\n      <th>296</th>\n      <th>297</th>\n      <th>298</th>\n      <th>299</th>\n    </tr>\n    <tr>\n      <th>word</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>,</th>\n      <td>-0.082752</td>\n      <td>0.672040</td>\n      <td>-0.14987</td>\n      <td>-0.064983</td>\n      <td>0.056491</td>\n      <td>0.402280</td>\n      <td>0.002775</td>\n      <td>-0.331100</td>\n      <td>-0.306910</td>\n      <td>2.0817</td>\n      <td>...</td>\n      <td>-0.14331</td>\n      <td>0.018267</td>\n      <td>-0.18643</td>\n      <td>0.207090</td>\n      <td>-0.355980</td>\n      <td>0.053380</td>\n      <td>-0.050821</td>\n      <td>-0.191800</td>\n      <td>-0.378460</td>\n      <td>-0.06589</td>\n    </tr>\n    <tr>\n      <th>.</th>\n      <td>0.012001</td>\n      <td>0.207510</td>\n      <td>-0.12578</td>\n      <td>-0.593250</td>\n      <td>0.125250</td>\n      <td>0.159750</td>\n      <td>0.137480</td>\n      <td>-0.331570</td>\n      <td>-0.136940</td>\n      <td>1.7893</td>\n      <td>...</td>\n      <td>0.16165</td>\n      <td>-0.066737</td>\n      <td>-0.29556</td>\n      <td>0.022612</td>\n      <td>-0.281350</td>\n      <td>0.063500</td>\n      <td>0.140190</td>\n      <td>0.138710</td>\n      <td>-0.360490</td>\n      <td>-0.03500</td>\n    </tr>\n    <tr>\n      <th>the</th>\n      <td>0.272040</td>\n      <td>-0.062030</td>\n      <td>-0.18840</td>\n      <td>0.023225</td>\n      <td>-0.018158</td>\n      <td>0.006719</td>\n      <td>-0.138770</td>\n      <td>0.177080</td>\n      <td>0.177090</td>\n      <td>2.5882</td>\n      <td>...</td>\n      <td>-0.42810</td>\n      <td>0.168990</td>\n      <td>0.22511</td>\n      <td>-0.285570</td>\n      <td>-0.102800</td>\n      <td>-0.018168</td>\n      <td>0.114070</td>\n      <td>0.130150</td>\n      <td>-0.183170</td>\n      <td>0.13230</td>\n    </tr>\n    <tr>\n      <th>and</th>\n      <td>-0.185670</td>\n      <td>0.066008</td>\n      <td>-0.25209</td>\n      <td>-0.117250</td>\n      <td>0.265130</td>\n      <td>0.064908</td>\n      <td>0.122910</td>\n      <td>-0.093979</td>\n      <td>0.024321</td>\n      <td>2.4926</td>\n      <td>...</td>\n      <td>-0.59396</td>\n      <td>-0.097729</td>\n      <td>0.20072</td>\n      <td>0.170550</td>\n      <td>-0.004736</td>\n      <td>-0.039709</td>\n      <td>0.324980</td>\n      <td>-0.023452</td>\n      <td>0.123020</td>\n      <td>0.33120</td>\n    </tr>\n    <tr>\n      <th>to</th>\n      <td>0.319240</td>\n      <td>0.063160</td>\n      <td>-0.27858</td>\n      <td>0.261200</td>\n      <td>0.079248</td>\n      <td>-0.214620</td>\n      <td>-0.104950</td>\n      <td>0.154950</td>\n      <td>-0.033530</td>\n      <td>2.4834</td>\n      <td>...</td>\n      <td>-0.12977</td>\n      <td>0.371300</td>\n      <td>0.18888</td>\n      <td>-0.004274</td>\n      <td>-0.106450</td>\n      <td>-0.258100</td>\n      <td>-0.044629</td>\n      <td>0.082745</td>\n      <td>0.097801</td>\n      <td>0.25045</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 300 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "source_embedding.index.rename('word', inplace=True)\n",
    "source_embedding.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<class 'pandas.core.frame.DataFrame'>\nIndex: 2196017 entries, , to zulchzulu\nColumns: 300 entries, 0 to 299\ndtypes: float64(300)\nmemory usage: 4.9+ GB\n"
    }
   ],
   "source": [
    "source_embedding.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                   int\nword                  \nthe                  1\nbrands               2\nqueen                3\nelizabeth            4\n,                    5\n...                ...\nkhizr            44771\nedwards-helaire  44772\nkorg             44773\nminilogue        44774\n78-55            44775\n\n[44775 rows x 1 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>int</th>\n    </tr>\n    <tr>\n      <th>word</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>the</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>brands</th>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>queen</th>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>elizabeth</th>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>,</th>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>khizr</th>\n      <td>44771</td>\n    </tr>\n    <tr>\n      <th>edwards-helaire</th>\n      <td>44772</td>\n    </tr>\n    <tr>\n      <th>korg</th>\n      <td>44773</td>\n    </tr>\n    <tr>\n      <th>minilogue</th>\n      <td>44774</td>\n    </tr>\n    <tr>\n      <th>78-55</th>\n      <td>44775</td>\n    </tr>\n  </tbody>\n</table>\n<p>44775 rows × 1 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "word2int = pd.read_table('/Users/baeyuna/Documents/SNU_DLab/NAML/data/train/word2int.tsv', na_filter=False, index_col='word')\n",
    "word2int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[ 0.02291545,  0.01740933,  0.01044573, ...,  0.02647595,\n        -0.05486389,  0.001196  ],\n       [ 0.02087866,  0.03977818,  0.02973026, ..., -0.04849208,\n        -0.03428288, -0.03353313],\n       [-0.00754663,  0.00295051, -0.00115429, ..., -0.00839342,\n         0.01157734, -0.03211762],\n       ...,\n       [ 0.03940183, -0.02765681, -0.01243274, ...,  0.01878955,\n        -0.01772225,  0.05469738],\n       [-0.00503979,  0.01623228,  0.06277147, ..., -0.0290992 ,\n        -0.03663587,  0.01084558],\n       [-0.01480096,  0.0230887 , -0.04091615, ..., -0.04485734,\n        -0.05254643, -0.02245559]], dtype=float32)"
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "word_embeddings = np.load('/Users/baeyuna/Documents/SNU_DLab/TANR_Bert/data/glove/word_embeddings.npy')\n",
    "word_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(44774, 768)"
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "word_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "            0         1         2         3         4         5         6    \\\nwords                                                                         \n0      0.022915  0.017409  0.010446 -0.048069  0.032508  0.013682 -0.043972   \n1      0.020879  0.039778  0.029730 -0.002456  0.059781 -0.025582 -0.029728   \n2     -0.007547  0.002951 -0.001154 -0.047338  0.037052  0.011151 -0.070805   \n3      0.019542 -0.045189  0.028748 -0.028304  0.034132  0.049069 -0.076122   \n4     -0.021864  0.011111  0.016383 -0.032495 -0.002032  0.032761 -0.019221   \n...         ...       ...       ...       ...       ...       ...       ...   \n44769  0.039653 -0.058133 -0.060750 -0.072139  0.026598  0.031439  0.009851   \n44770  0.040825 -0.037304  0.005907 -0.023890 -0.007193 -0.037433 -0.071384   \n44771  0.039402 -0.027657 -0.012433 -0.051315  0.026456 -0.026098 -0.015726   \n44772 -0.005040  0.016232  0.062771 -0.027899 -0.003462 -0.002256 -0.033712   \n44773 -0.014801  0.023089 -0.040916 -0.032936 -0.011149 -0.044840 -0.025739   \n\n            7         8         9    ...       758       759       760  \\\nwords                                ...                                 \n0      0.028114 -0.081688 -0.003506  ... -0.014207  0.003513 -0.056283   \n1     -0.038067 -0.051656  0.040926  ... -0.004319  0.058174 -0.023548   \n2     -0.029634 -0.012547 -0.008933  ...  0.030856  0.036562 -0.056421   \n3      0.023633 -0.062191 -0.022393  ...  0.014009  0.046649 -0.040938   \n4     -0.006823 -0.076860 -0.039014  ...  0.000077  0.017377 -0.059505   \n...         ...       ...       ...  ...       ...       ...       ...   \n44769  0.040807 -0.045499 -0.059950  ...  0.047280  0.013840 -0.034338   \n44770 -0.007603 -0.018867  0.004944  ...  0.046952  0.050125  0.060447   \n44771  0.053842 -0.047304 -0.019760  ...  0.005184  0.019469 -0.029712   \n44772  0.025692  0.042241 -0.061289  ...  0.078346  0.022015 -0.045793   \n44773 -0.057561 -0.059291 -0.053538  ...  0.009649 -0.006906 -0.019531   \n\n            761       762       763       764       765       766       767  \nwords                                                                        \n0      0.023020  0.047154  0.024799  0.025471  0.026476 -0.054864  0.001196  \n1     -0.041602  0.001282  0.009522  0.026286 -0.048492 -0.034283 -0.033533  \n2     -0.027629  0.028740  0.003800  0.027521 -0.008393  0.011577 -0.032118  \n3     -0.059904 -0.022907  0.016196 -0.041572 -0.009329 -0.002110 -0.034657  \n4      0.064820  0.065166 -0.003043 -0.019843  0.010483 -0.066509 -0.006312  \n...         ...       ...       ...       ...       ...       ...       ...  \n44769  0.003750 -0.042403 -0.041079 -0.023069  0.008094 -0.011244  0.047785  \n44770 -0.088661  0.059591  0.013882  0.007241  0.041523 -0.038662  0.019831  \n44771  0.017312 -0.042999 -0.055916  0.010879  0.018790 -0.017722  0.054697  \n44772  0.005103  0.027438  0.003614  0.043436 -0.029099 -0.036636  0.010846  \n44773 -0.048854 -0.013579 -0.053662  0.025897 -0.044857 -0.052546 -0.022456  \n\n[44774 rows x 768 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>758</th>\n      <th>759</th>\n      <th>760</th>\n      <th>761</th>\n      <th>762</th>\n      <th>763</th>\n      <th>764</th>\n      <th>765</th>\n      <th>766</th>\n      <th>767</th>\n    </tr>\n    <tr>\n      <th>words</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.022915</td>\n      <td>0.017409</td>\n      <td>0.010446</td>\n      <td>-0.048069</td>\n      <td>0.032508</td>\n      <td>0.013682</td>\n      <td>-0.043972</td>\n      <td>0.028114</td>\n      <td>-0.081688</td>\n      <td>-0.003506</td>\n      <td>...</td>\n      <td>-0.014207</td>\n      <td>0.003513</td>\n      <td>-0.056283</td>\n      <td>0.023020</td>\n      <td>0.047154</td>\n      <td>0.024799</td>\n      <td>0.025471</td>\n      <td>0.026476</td>\n      <td>-0.054864</td>\n      <td>0.001196</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.020879</td>\n      <td>0.039778</td>\n      <td>0.029730</td>\n      <td>-0.002456</td>\n      <td>0.059781</td>\n      <td>-0.025582</td>\n      <td>-0.029728</td>\n      <td>-0.038067</td>\n      <td>-0.051656</td>\n      <td>0.040926</td>\n      <td>...</td>\n      <td>-0.004319</td>\n      <td>0.058174</td>\n      <td>-0.023548</td>\n      <td>-0.041602</td>\n      <td>0.001282</td>\n      <td>0.009522</td>\n      <td>0.026286</td>\n      <td>-0.048492</td>\n      <td>-0.034283</td>\n      <td>-0.033533</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-0.007547</td>\n      <td>0.002951</td>\n      <td>-0.001154</td>\n      <td>-0.047338</td>\n      <td>0.037052</td>\n      <td>0.011151</td>\n      <td>-0.070805</td>\n      <td>-0.029634</td>\n      <td>-0.012547</td>\n      <td>-0.008933</td>\n      <td>...</td>\n      <td>0.030856</td>\n      <td>0.036562</td>\n      <td>-0.056421</td>\n      <td>-0.027629</td>\n      <td>0.028740</td>\n      <td>0.003800</td>\n      <td>0.027521</td>\n      <td>-0.008393</td>\n      <td>0.011577</td>\n      <td>-0.032118</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.019542</td>\n      <td>-0.045189</td>\n      <td>0.028748</td>\n      <td>-0.028304</td>\n      <td>0.034132</td>\n      <td>0.049069</td>\n      <td>-0.076122</td>\n      <td>0.023633</td>\n      <td>-0.062191</td>\n      <td>-0.022393</td>\n      <td>...</td>\n      <td>0.014009</td>\n      <td>0.046649</td>\n      <td>-0.040938</td>\n      <td>-0.059904</td>\n      <td>-0.022907</td>\n      <td>0.016196</td>\n      <td>-0.041572</td>\n      <td>-0.009329</td>\n      <td>-0.002110</td>\n      <td>-0.034657</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-0.021864</td>\n      <td>0.011111</td>\n      <td>0.016383</td>\n      <td>-0.032495</td>\n      <td>-0.002032</td>\n      <td>0.032761</td>\n      <td>-0.019221</td>\n      <td>-0.006823</td>\n      <td>-0.076860</td>\n      <td>-0.039014</td>\n      <td>...</td>\n      <td>0.000077</td>\n      <td>0.017377</td>\n      <td>-0.059505</td>\n      <td>0.064820</td>\n      <td>0.065166</td>\n      <td>-0.003043</td>\n      <td>-0.019843</td>\n      <td>0.010483</td>\n      <td>-0.066509</td>\n      <td>-0.006312</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>44769</th>\n      <td>0.039653</td>\n      <td>-0.058133</td>\n      <td>-0.060750</td>\n      <td>-0.072139</td>\n      <td>0.026598</td>\n      <td>0.031439</td>\n      <td>0.009851</td>\n      <td>0.040807</td>\n      <td>-0.045499</td>\n      <td>-0.059950</td>\n      <td>...</td>\n      <td>0.047280</td>\n      <td>0.013840</td>\n      <td>-0.034338</td>\n      <td>0.003750</td>\n      <td>-0.042403</td>\n      <td>-0.041079</td>\n      <td>-0.023069</td>\n      <td>0.008094</td>\n      <td>-0.011244</td>\n      <td>0.047785</td>\n    </tr>\n    <tr>\n      <th>44770</th>\n      <td>0.040825</td>\n      <td>-0.037304</td>\n      <td>0.005907</td>\n      <td>-0.023890</td>\n      <td>-0.007193</td>\n      <td>-0.037433</td>\n      <td>-0.071384</td>\n      <td>-0.007603</td>\n      <td>-0.018867</td>\n      <td>0.004944</td>\n      <td>...</td>\n      <td>0.046952</td>\n      <td>0.050125</td>\n      <td>0.060447</td>\n      <td>-0.088661</td>\n      <td>0.059591</td>\n      <td>0.013882</td>\n      <td>0.007241</td>\n      <td>0.041523</td>\n      <td>-0.038662</td>\n      <td>0.019831</td>\n    </tr>\n    <tr>\n      <th>44771</th>\n      <td>0.039402</td>\n      <td>-0.027657</td>\n      <td>-0.012433</td>\n      <td>-0.051315</td>\n      <td>0.026456</td>\n      <td>-0.026098</td>\n      <td>-0.015726</td>\n      <td>0.053842</td>\n      <td>-0.047304</td>\n      <td>-0.019760</td>\n      <td>...</td>\n      <td>0.005184</td>\n      <td>0.019469</td>\n      <td>-0.029712</td>\n      <td>0.017312</td>\n      <td>-0.042999</td>\n      <td>-0.055916</td>\n      <td>0.010879</td>\n      <td>0.018790</td>\n      <td>-0.017722</td>\n      <td>0.054697</td>\n    </tr>\n    <tr>\n      <th>44772</th>\n      <td>-0.005040</td>\n      <td>0.016232</td>\n      <td>0.062771</td>\n      <td>-0.027899</td>\n      <td>-0.003462</td>\n      <td>-0.002256</td>\n      <td>-0.033712</td>\n      <td>0.025692</td>\n      <td>0.042241</td>\n      <td>-0.061289</td>\n      <td>...</td>\n      <td>0.078346</td>\n      <td>0.022015</td>\n      <td>-0.045793</td>\n      <td>0.005103</td>\n      <td>0.027438</td>\n      <td>0.003614</td>\n      <td>0.043436</td>\n      <td>-0.029099</td>\n      <td>-0.036636</td>\n      <td>0.010846</td>\n    </tr>\n    <tr>\n      <th>44773</th>\n      <td>-0.014801</td>\n      <td>0.023089</td>\n      <td>-0.040916</td>\n      <td>-0.032936</td>\n      <td>-0.011149</td>\n      <td>-0.044840</td>\n      <td>-0.025739</td>\n      <td>-0.057561</td>\n      <td>-0.059291</td>\n      <td>-0.053538</td>\n      <td>...</td>\n      <td>0.009649</td>\n      <td>-0.006906</td>\n      <td>-0.019531</td>\n      <td>-0.048854</td>\n      <td>-0.013579</td>\n      <td>-0.053662</td>\n      <td>0.025897</td>\n      <td>-0.044857</td>\n      <td>-0.052546</td>\n      <td>-0.022456</td>\n    </tr>\n  </tbody>\n</table>\n<p>44774 rows × 768 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "df = pd.DataFrame(word_embeddings)\n",
    "df.loc[:,'words'] = word2int.index.tolist\n",
    "df = df.rename_axis('words').iloc[:,:-1]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "            0         1         2         3         4         5         6    \\\nint                                                                           \n813   -0.265540  0.335310  0.218600 -0.301000 -0.055470 -0.242360  0.172360   \n4518  -0.444670  0.695360  0.427480  0.219060  0.117570 -0.213110  0.580860   \n497   -0.607120  0.425440  0.510400 -0.287500  0.514750  0.082824 -0.415860   \n2968  -1.257500  0.716480 -0.065293  0.006738 -0.279670 -0.068012 -0.093761   \n903   -0.329710  0.334650 -0.067601 -0.032601  0.757840 -0.173780  0.321860   \n...         ...       ...       ...       ...       ...       ...       ...   \n23009 -0.281300 -0.061073 -0.028948 -0.072932  0.088378  0.114800  0.067462   \n24858 -0.410340  0.672390  0.122260  0.326450 -0.282560 -0.084786  0.012697   \n1548   0.001631 -0.065206 -0.280480  0.045933 -0.051209  0.091673  0.097920   \n18912 -0.370110 -0.244340  0.074146  0.323370 -0.237270 -0.500430  0.344150   \n43187 -0.394530 -1.640900  0.082741  0.617910 -0.335900 -0.231500  0.623460   \n\n            7         8        9    ...       290       291       292  \\\nint                                 ...                                 \n813   -0.163340 -0.109000  1.26710  ... -0.386450 -0.150560 -0.032827   \n4518   0.073145 -0.085245  0.36655  ...  0.281160  0.357600 -0.056253   \n497   -0.343850  0.489610  0.70518  ... -0.302490 -0.025315  0.471890   \n2968   0.316350  0.159900  0.77747  ...  0.155690  0.349450  0.160910   \n903   -0.079652  0.182100  0.66829  ... -0.716630 -0.158360  0.018979   \n...         ...       ...      ...  ...       ...       ...       ...   \n23009 -0.228190  0.309690  0.56512  ...  0.190860  0.166820 -0.610980   \n24858 -0.087197 -0.049720  0.62355  ... -0.247020  0.537520 -0.175870   \n1548  -0.114860  0.367430  1.21310  ... -0.279280  0.025629 -0.118460   \n18912  0.266310  0.075413 -0.20418  ...  0.086048 -0.096603 -0.314720   \n43187  0.064233 -0.111450 -1.57140  ... -0.380640 -1.085400 -0.403970   \n\n            293      294       295       296       297       298       299  \nint                                                                         \n813   -0.105290  0.28397 -0.255000  0.151950 -0.178590 -0.062878  0.162320  \n4518   0.219490  0.35125 -0.413770 -0.278680 -0.079391 -0.527170 -0.124130  \n497   -0.226380 -0.97497  0.061226 -0.388950 -0.185880  0.025965 -0.482310  \n2968  -0.296250 -0.32933 -0.447430 -0.864720 -0.375350  0.111350 -0.332260  \n903    0.273480  0.23848  0.249300  0.401640 -0.264940  0.166040  0.220470  \n...         ...      ...       ...       ...       ...       ...       ...  \n23009 -0.409540 -0.11875  0.007000  0.512030  0.028562 -0.152240  0.075667  \n24858  0.076204 -0.20105 -0.202310 -0.118580  0.096621  0.504210 -0.222980  \n1548  -0.434610 -0.00727 -0.136660  0.399090  0.012960  0.000707 -0.066269  \n18912 -0.189210 -0.09357  0.008059 -0.026633 -0.083889  0.476050  0.382670  \n43187  0.260690 -0.45591  0.344540  0.513370  0.258720 -1.458500  0.167150  \n\n[39234 rows x 300 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>290</th>\n      <th>291</th>\n      <th>292</th>\n      <th>293</th>\n      <th>294</th>\n      <th>295</th>\n      <th>296</th>\n      <th>297</th>\n      <th>298</th>\n      <th>299</th>\n    </tr>\n    <tr>\n      <th>int</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>813</th>\n      <td>-0.265540</td>\n      <td>0.335310</td>\n      <td>0.218600</td>\n      <td>-0.301000</td>\n      <td>-0.055470</td>\n      <td>-0.242360</td>\n      <td>0.172360</td>\n      <td>-0.163340</td>\n      <td>-0.109000</td>\n      <td>1.26710</td>\n      <td>...</td>\n      <td>-0.386450</td>\n      <td>-0.150560</td>\n      <td>-0.032827</td>\n      <td>-0.105290</td>\n      <td>0.28397</td>\n      <td>-0.255000</td>\n      <td>0.151950</td>\n      <td>-0.178590</td>\n      <td>-0.062878</td>\n      <td>0.162320</td>\n    </tr>\n    <tr>\n      <th>4518</th>\n      <td>-0.444670</td>\n      <td>0.695360</td>\n      <td>0.427480</td>\n      <td>0.219060</td>\n      <td>0.117570</td>\n      <td>-0.213110</td>\n      <td>0.580860</td>\n      <td>0.073145</td>\n      <td>-0.085245</td>\n      <td>0.36655</td>\n      <td>...</td>\n      <td>0.281160</td>\n      <td>0.357600</td>\n      <td>-0.056253</td>\n      <td>0.219490</td>\n      <td>0.35125</td>\n      <td>-0.413770</td>\n      <td>-0.278680</td>\n      <td>-0.079391</td>\n      <td>-0.527170</td>\n      <td>-0.124130</td>\n    </tr>\n    <tr>\n      <th>497</th>\n      <td>-0.607120</td>\n      <td>0.425440</td>\n      <td>0.510400</td>\n      <td>-0.287500</td>\n      <td>0.514750</td>\n      <td>0.082824</td>\n      <td>-0.415860</td>\n      <td>-0.343850</td>\n      <td>0.489610</td>\n      <td>0.70518</td>\n      <td>...</td>\n      <td>-0.302490</td>\n      <td>-0.025315</td>\n      <td>0.471890</td>\n      <td>-0.226380</td>\n      <td>-0.97497</td>\n      <td>0.061226</td>\n      <td>-0.388950</td>\n      <td>-0.185880</td>\n      <td>0.025965</td>\n      <td>-0.482310</td>\n    </tr>\n    <tr>\n      <th>2968</th>\n      <td>-1.257500</td>\n      <td>0.716480</td>\n      <td>-0.065293</td>\n      <td>0.006738</td>\n      <td>-0.279670</td>\n      <td>-0.068012</td>\n      <td>-0.093761</td>\n      <td>0.316350</td>\n      <td>0.159900</td>\n      <td>0.77747</td>\n      <td>...</td>\n      <td>0.155690</td>\n      <td>0.349450</td>\n      <td>0.160910</td>\n      <td>-0.296250</td>\n      <td>-0.32933</td>\n      <td>-0.447430</td>\n      <td>-0.864720</td>\n      <td>-0.375350</td>\n      <td>0.111350</td>\n      <td>-0.332260</td>\n    </tr>\n    <tr>\n      <th>903</th>\n      <td>-0.329710</td>\n      <td>0.334650</td>\n      <td>-0.067601</td>\n      <td>-0.032601</td>\n      <td>0.757840</td>\n      <td>-0.173780</td>\n      <td>0.321860</td>\n      <td>-0.079652</td>\n      <td>0.182100</td>\n      <td>0.66829</td>\n      <td>...</td>\n      <td>-0.716630</td>\n      <td>-0.158360</td>\n      <td>0.018979</td>\n      <td>0.273480</td>\n      <td>0.23848</td>\n      <td>0.249300</td>\n      <td>0.401640</td>\n      <td>-0.264940</td>\n      <td>0.166040</td>\n      <td>0.220470</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>23009</th>\n      <td>-0.281300</td>\n      <td>-0.061073</td>\n      <td>-0.028948</td>\n      <td>-0.072932</td>\n      <td>0.088378</td>\n      <td>0.114800</td>\n      <td>0.067462</td>\n      <td>-0.228190</td>\n      <td>0.309690</td>\n      <td>0.56512</td>\n      <td>...</td>\n      <td>0.190860</td>\n      <td>0.166820</td>\n      <td>-0.610980</td>\n      <td>-0.409540</td>\n      <td>-0.11875</td>\n      <td>0.007000</td>\n      <td>0.512030</td>\n      <td>0.028562</td>\n      <td>-0.152240</td>\n      <td>0.075667</td>\n    </tr>\n    <tr>\n      <th>24858</th>\n      <td>-0.410340</td>\n      <td>0.672390</td>\n      <td>0.122260</td>\n      <td>0.326450</td>\n      <td>-0.282560</td>\n      <td>-0.084786</td>\n      <td>0.012697</td>\n      <td>-0.087197</td>\n      <td>-0.049720</td>\n      <td>0.62355</td>\n      <td>...</td>\n      <td>-0.247020</td>\n      <td>0.537520</td>\n      <td>-0.175870</td>\n      <td>0.076204</td>\n      <td>-0.20105</td>\n      <td>-0.202310</td>\n      <td>-0.118580</td>\n      <td>0.096621</td>\n      <td>0.504210</td>\n      <td>-0.222980</td>\n    </tr>\n    <tr>\n      <th>1548</th>\n      <td>0.001631</td>\n      <td>-0.065206</td>\n      <td>-0.280480</td>\n      <td>0.045933</td>\n      <td>-0.051209</td>\n      <td>0.091673</td>\n      <td>0.097920</td>\n      <td>-0.114860</td>\n      <td>0.367430</td>\n      <td>1.21310</td>\n      <td>...</td>\n      <td>-0.279280</td>\n      <td>0.025629</td>\n      <td>-0.118460</td>\n      <td>-0.434610</td>\n      <td>-0.00727</td>\n      <td>-0.136660</td>\n      <td>0.399090</td>\n      <td>0.012960</td>\n      <td>0.000707</td>\n      <td>-0.066269</td>\n    </tr>\n    <tr>\n      <th>18912</th>\n      <td>-0.370110</td>\n      <td>-0.244340</td>\n      <td>0.074146</td>\n      <td>0.323370</td>\n      <td>-0.237270</td>\n      <td>-0.500430</td>\n      <td>0.344150</td>\n      <td>0.266310</td>\n      <td>0.075413</td>\n      <td>-0.20418</td>\n      <td>...</td>\n      <td>0.086048</td>\n      <td>-0.096603</td>\n      <td>-0.314720</td>\n      <td>-0.189210</td>\n      <td>-0.09357</td>\n      <td>0.008059</td>\n      <td>-0.026633</td>\n      <td>-0.083889</td>\n      <td>0.476050</td>\n      <td>0.382670</td>\n    </tr>\n    <tr>\n      <th>43187</th>\n      <td>-0.394530</td>\n      <td>-1.640900</td>\n      <td>0.082741</td>\n      <td>0.617910</td>\n      <td>-0.335900</td>\n      <td>-0.231500</td>\n      <td>0.623460</td>\n      <td>0.064233</td>\n      <td>-0.111450</td>\n      <td>-1.57140</td>\n      <td>...</td>\n      <td>-0.380640</td>\n      <td>-1.085400</td>\n      <td>-0.403970</td>\n      <td>0.260690</td>\n      <td>-0.45591</td>\n      <td>0.344540</td>\n      <td>0.513370</td>\n      <td>0.258720</td>\n      <td>-1.458500</td>\n      <td>0.167150</td>\n    </tr>\n  </tbody>\n</table>\n<p>39234 rows × 300 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "merged = word2int.merge(source_embedding,\n",
    "                            how='inner',\n",
    "                            left_index=True,\n",
    "                            right_index=True)\n",
    "merged.set_index('int', inplace=True)\n",
    "merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<class 'pandas.core.frame.DataFrame'>\nInt64Index: 39234 entries, 813 to 43187\nColumns: 300 entries, 0 to 299\ndtypes: float64(300)\nmemory usage: 90.1 MB\n"
    }
   ],
   "source": [
    "merged.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "            0         1         2         3         4         5         6    \\\nint                                                                           \n0      1.288098 -0.063775  1.497028  1.547180 -0.593917 -0.678326  0.378858   \n307    0.166003  0.406335 -1.098457  1.818031  0.053056 -1.215062  0.795894   \n500   -1.322150 -0.126595  0.550409 -0.927481 -0.585284 -0.160521 -0.867233   \n513    0.483195 -1.620398  0.741474  1.312880 -0.148232 -0.529110 -0.572087   \n1028   1.904152 -0.605848  0.856021  0.580689 -0.309649  0.839725 -1.027478   \n...         ...       ...       ...       ...       ...       ...       ...   \n44766  0.383020  0.841610  0.005598 -0.739981 -0.818388 -0.202650 -0.194423   \n44770  1.422377 -1.105946 -1.645423  0.132622 -0.917275  1.720527  1.408529   \n44771  1.167710  0.501093  2.651376  0.263755 -1.639525  1.472220 -0.488728   \n44772  0.624246 -0.738757  1.398356 -1.661943 -0.269494  0.198956 -1.096041   \n44774 -1.023568  0.212542  0.814289 -0.392822  0.187149  0.234141 -1.478426   \n\n            7         8         9    ...       290       291       292  \\\nint                                  ...                                 \n0     -0.926503  0.099760 -0.022329  ...  0.443984  1.376088  0.680306   \n307   -0.676047 -1.399250  1.016314  ... -0.341438 -1.036171 -0.898232   \n500    0.658533  0.297369 -0.703974  ...  0.350634  0.823332  0.266226   \n513   -1.498019 -0.425934  0.748248  ... -0.289518  0.538498  0.247436   \n1028   0.869661  0.117237  1.087761  ... -0.661081 -0.535649  1.505538   \n...         ...       ...       ...  ...       ...       ...       ...   \n44766  1.528221 -0.962780 -0.017733  ...  0.470464  1.727420 -1.434436   \n44770  0.039046  0.230192  2.481090  ...  1.202908  0.132700 -0.779296   \n44771  0.828530 -0.137972 -0.864210  ... -0.177001 -0.015481  0.133913   \n44772 -0.110934 -0.406452  1.145348  ...  1.560792 -0.380519  0.558356   \n44774  1.127849  0.991175  0.810681  ... -0.813977  0.370189  1.475328   \n\n            293       294       295       296       297       298       299  \nint                                                                          \n0      0.547370  2.090382 -0.813310  1.020213 -0.344693  0.615885  0.518399  \n307    0.223727 -0.170784  0.142636 -1.328306  0.227220  1.442203  1.559786  \n500   -1.003233  0.571318  1.115777 -0.726129 -0.503877 -0.512246  2.049852  \n513    0.182574 -1.838674  0.840061  0.421991 -2.460203 -2.859892  1.767834  \n1028   0.428832  0.205585  0.210413  2.599663  0.206422  0.236708 -1.410028  \n...         ...       ...       ...       ...       ...       ...       ...  \n44766 -1.053443  1.026610  0.806091 -1.263353  1.413617 -0.267850  1.174025  \n44770 -0.593796 -1.080095 -0.418093 -0.598790 -0.287385 -0.444510  0.589174  \n44771 -0.160410 -0.539696 -0.280225  0.887341 -0.850246 -0.676386  1.590958  \n44772  0.316166  0.561124  0.794218  0.888252 -0.415748 -0.717195 -0.602704  \n44774 -0.414379 -0.031168  1.191243 -1.942284 -0.867700 -0.075390 -1.400550  \n\n[5542 rows x 300 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>290</th>\n      <th>291</th>\n      <th>292</th>\n      <th>293</th>\n      <th>294</th>\n      <th>295</th>\n      <th>296</th>\n      <th>297</th>\n      <th>298</th>\n      <th>299</th>\n    </tr>\n    <tr>\n      <th>int</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.288098</td>\n      <td>-0.063775</td>\n      <td>1.497028</td>\n      <td>1.547180</td>\n      <td>-0.593917</td>\n      <td>-0.678326</td>\n      <td>0.378858</td>\n      <td>-0.926503</td>\n      <td>0.099760</td>\n      <td>-0.022329</td>\n      <td>...</td>\n      <td>0.443984</td>\n      <td>1.376088</td>\n      <td>0.680306</td>\n      <td>0.547370</td>\n      <td>2.090382</td>\n      <td>-0.813310</td>\n      <td>1.020213</td>\n      <td>-0.344693</td>\n      <td>0.615885</td>\n      <td>0.518399</td>\n    </tr>\n    <tr>\n      <th>307</th>\n      <td>0.166003</td>\n      <td>0.406335</td>\n      <td>-1.098457</td>\n      <td>1.818031</td>\n      <td>0.053056</td>\n      <td>-1.215062</td>\n      <td>0.795894</td>\n      <td>-0.676047</td>\n      <td>-1.399250</td>\n      <td>1.016314</td>\n      <td>...</td>\n      <td>-0.341438</td>\n      <td>-1.036171</td>\n      <td>-0.898232</td>\n      <td>0.223727</td>\n      <td>-0.170784</td>\n      <td>0.142636</td>\n      <td>-1.328306</td>\n      <td>0.227220</td>\n      <td>1.442203</td>\n      <td>1.559786</td>\n    </tr>\n    <tr>\n      <th>500</th>\n      <td>-1.322150</td>\n      <td>-0.126595</td>\n      <td>0.550409</td>\n      <td>-0.927481</td>\n      <td>-0.585284</td>\n      <td>-0.160521</td>\n      <td>-0.867233</td>\n      <td>0.658533</td>\n      <td>0.297369</td>\n      <td>-0.703974</td>\n      <td>...</td>\n      <td>0.350634</td>\n      <td>0.823332</td>\n      <td>0.266226</td>\n      <td>-1.003233</td>\n      <td>0.571318</td>\n      <td>1.115777</td>\n      <td>-0.726129</td>\n      <td>-0.503877</td>\n      <td>-0.512246</td>\n      <td>2.049852</td>\n    </tr>\n    <tr>\n      <th>513</th>\n      <td>0.483195</td>\n      <td>-1.620398</td>\n      <td>0.741474</td>\n      <td>1.312880</td>\n      <td>-0.148232</td>\n      <td>-0.529110</td>\n      <td>-0.572087</td>\n      <td>-1.498019</td>\n      <td>-0.425934</td>\n      <td>0.748248</td>\n      <td>...</td>\n      <td>-0.289518</td>\n      <td>0.538498</td>\n      <td>0.247436</td>\n      <td>0.182574</td>\n      <td>-1.838674</td>\n      <td>0.840061</td>\n      <td>0.421991</td>\n      <td>-2.460203</td>\n      <td>-2.859892</td>\n      <td>1.767834</td>\n    </tr>\n    <tr>\n      <th>1028</th>\n      <td>1.904152</td>\n      <td>-0.605848</td>\n      <td>0.856021</td>\n      <td>0.580689</td>\n      <td>-0.309649</td>\n      <td>0.839725</td>\n      <td>-1.027478</td>\n      <td>0.869661</td>\n      <td>0.117237</td>\n      <td>1.087761</td>\n      <td>...</td>\n      <td>-0.661081</td>\n      <td>-0.535649</td>\n      <td>1.505538</td>\n      <td>0.428832</td>\n      <td>0.205585</td>\n      <td>0.210413</td>\n      <td>2.599663</td>\n      <td>0.206422</td>\n      <td>0.236708</td>\n      <td>-1.410028</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>44766</th>\n      <td>0.383020</td>\n      <td>0.841610</td>\n      <td>0.005598</td>\n      <td>-0.739981</td>\n      <td>-0.818388</td>\n      <td>-0.202650</td>\n      <td>-0.194423</td>\n      <td>1.528221</td>\n      <td>-0.962780</td>\n      <td>-0.017733</td>\n      <td>...</td>\n      <td>0.470464</td>\n      <td>1.727420</td>\n      <td>-1.434436</td>\n      <td>-1.053443</td>\n      <td>1.026610</td>\n      <td>0.806091</td>\n      <td>-1.263353</td>\n      <td>1.413617</td>\n      <td>-0.267850</td>\n      <td>1.174025</td>\n    </tr>\n    <tr>\n      <th>44770</th>\n      <td>1.422377</td>\n      <td>-1.105946</td>\n      <td>-1.645423</td>\n      <td>0.132622</td>\n      <td>-0.917275</td>\n      <td>1.720527</td>\n      <td>1.408529</td>\n      <td>0.039046</td>\n      <td>0.230192</td>\n      <td>2.481090</td>\n      <td>...</td>\n      <td>1.202908</td>\n      <td>0.132700</td>\n      <td>-0.779296</td>\n      <td>-0.593796</td>\n      <td>-1.080095</td>\n      <td>-0.418093</td>\n      <td>-0.598790</td>\n      <td>-0.287385</td>\n      <td>-0.444510</td>\n      <td>0.589174</td>\n    </tr>\n    <tr>\n      <th>44771</th>\n      <td>1.167710</td>\n      <td>0.501093</td>\n      <td>2.651376</td>\n      <td>0.263755</td>\n      <td>-1.639525</td>\n      <td>1.472220</td>\n      <td>-0.488728</td>\n      <td>0.828530</td>\n      <td>-0.137972</td>\n      <td>-0.864210</td>\n      <td>...</td>\n      <td>-0.177001</td>\n      <td>-0.015481</td>\n      <td>0.133913</td>\n      <td>-0.160410</td>\n      <td>-0.539696</td>\n      <td>-0.280225</td>\n      <td>0.887341</td>\n      <td>-0.850246</td>\n      <td>-0.676386</td>\n      <td>1.590958</td>\n    </tr>\n    <tr>\n      <th>44772</th>\n      <td>0.624246</td>\n      <td>-0.738757</td>\n      <td>1.398356</td>\n      <td>-1.661943</td>\n      <td>-0.269494</td>\n      <td>0.198956</td>\n      <td>-1.096041</td>\n      <td>-0.110934</td>\n      <td>-0.406452</td>\n      <td>1.145348</td>\n      <td>...</td>\n      <td>1.560792</td>\n      <td>-0.380519</td>\n      <td>0.558356</td>\n      <td>0.316166</td>\n      <td>0.561124</td>\n      <td>0.794218</td>\n      <td>0.888252</td>\n      <td>-0.415748</td>\n      <td>-0.717195</td>\n      <td>-0.602704</td>\n    </tr>\n    <tr>\n      <th>44774</th>\n      <td>-1.023568</td>\n      <td>0.212542</td>\n      <td>0.814289</td>\n      <td>-0.392822</td>\n      <td>0.187149</td>\n      <td>0.234141</td>\n      <td>-1.478426</td>\n      <td>1.127849</td>\n      <td>0.991175</td>\n      <td>0.810681</td>\n      <td>...</td>\n      <td>-0.813977</td>\n      <td>0.370189</td>\n      <td>1.475328</td>\n      <td>-0.414379</td>\n      <td>-0.031168</td>\n      <td>1.191243</td>\n      <td>-1.942284</td>\n      <td>-0.867700</td>\n      <td>-0.075390</td>\n      <td>-1.400550</td>\n    </tr>\n  </tbody>\n</table>\n<p>5542 rows × 300 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "source": [
    "missed_index = np.setdiff1d(np.arange(len(word2int) + 1),\n",
    "                            merged.index.values)\n",
    "missed_embedding = pd.DataFrame(data=np.random.normal(\n",
    "    size=(len(missed_index), 768)))\n",
    "missed_embedding['int'] = missed_index\n",
    "missed_embedding.set_index('int', inplace=True)\n",
    "missed_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_embedding = pd.concat([merged, missed_embedding]).sort_index()\n",
    "np.save(target, final_embedding.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[-0.78346185,  0.39302635, -0.71049564, ...,  1.1914767 ,\n        -0.58732314, -1.08972927],\n       [ 0.27204   , -0.06203   , -0.1884    , ...,  0.13015   ,\n        -0.18317   ,  0.1323    ],\n       [ 0.30791   ,  0.12428   , -0.045487  , ..., -0.4962    ,\n         0.56847   ,  0.51508   ],\n       ...,\n       [ 0.041715  ,  0.85602   ,  0.031382  , ...,  0.75407   ,\n        -0.72133   ,  0.19725   ],\n       [-0.24537994,  0.24460628, -0.9640435 , ...,  0.70863113,\n         0.23182131,  0.37770902],\n       [-0.044587  , -0.37936   ,  0.40889   , ...,  0.8848    ,\n        -0.41805   , -0.21511   ]])"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "pretrained_word_embedding = np.load('/Users/baeyuna/Documents/SNU_DLab/NAML/data/train/pretrained_word_embedding.npy')\n",
    "pretrained_word_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "44775"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "word_lst = word2int.index.tolist()\n",
    "len(word_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bert WordEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "def get_model(model_url, max_seq_length):\n",
    "  labse_layer = hub.KerasLayer(model_url, trainable=True)\n",
    "\n",
    "  # Define input.\n",
    "  input_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
    "                                         name=\"input_word_ids\")\n",
    "  input_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
    "                                     name=\"input_mask\")\n",
    "  segment_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
    "                                      name=\"segment_ids\")\n",
    "\n",
    "  # LaBSE layer.\n",
    "  pooled_output,  _ = labse_layer([input_word_ids, input_mask, segment_ids])\n",
    "\n",
    "  # The embedding is l2 normalized.\n",
    "  pooled_output = tf.keras.layers.Lambda(\n",
    "      lambda x: tf.nn.l2_normalize(x, axis=1))(pooled_output)\n",
    "\n",
    "  # Define model.\n",
    "  return tf.keras.Model(\n",
    "        inputs=[input_word_ids, input_mask, segment_ids],\n",
    "        outputs=pooled_output), labse_layer\n",
    "\n",
    "max_seq_length = 64\n",
    "labse_model, labse_layer = get_model(\n",
    "    model_url=\"https://tfhub.dev/google/LaBSE/1\", max_seq_length=max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bert\n",
    "\n",
    "vocab_file = labse_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "do_lower_case = labse_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = bert.bert_tokenization.FullTokenizer(vocab_file, do_lower_case)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_input(input_strings, tokenizer, max_seq_length):\n",
    "\n",
    "  input_ids_all, input_mask_all, segment_ids_all = [], [], []\n",
    "  for input_string in input_strings:\n",
    "    # Tokenize input.\n",
    "    input_tokens = [\"[CLS]\"] + tokenizer.tokenize(input_string) + [\"[SEP]\"]\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(input_tokens)\n",
    "    sequence_length = min(len(input_ids), max_seq_length)\n",
    "\n",
    "    # Padding or truncation.\n",
    "    if len(input_ids) >= max_seq_length:\n",
    "      input_ids = input_ids[:max_seq_length]\n",
    "    else:\n",
    "      input_ids = input_ids + [0] * (max_seq_length - len(input_ids))\n",
    "\n",
    "    input_mask = [1] * sequence_length + [0] * (max_seq_length - sequence_length)\n",
    "\n",
    "    input_ids_all.append(input_ids)\n",
    "    input_mask_all.append(input_mask)\n",
    "    segment_ids_all.append([0] * max_seq_length)\n",
    "\n",
    "  return np.array(input_ids_all), np.array(input_mask_all), np.array(segment_ids_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(input_text):\n",
    "  input_ids, input_mask, segment_ids = create_input(\n",
    "    input_text, tokenizer, max_seq_length)\n",
    "  return labse_model([input_ids, input_mask, segment_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tf.Tensor: shape=(3, 768), dtype=float32, numpy=\narray([[-0.01026385, -0.00157138, -0.01473237, ...,  0.0494036 ,\n        -0.01296441, -0.03172865],\n       [-0.05280595, -0.00894449, -0.05583495, ..., -0.00754025,\n         0.01059245, -0.01610355],\n       [ 0.02716724, -0.00592896,  0.01995578, ..., -0.00215263,\n         0.00364844, -0.03194168]], dtype=float32)>"
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "english_sentences = [\"dog\", \"Puppies are nice.\", \"I enjoy taking long walks along the beach with my dog.\"]\n",
    "\n",
    "english_embeddings = encode(english_sentences)\n",
    "english_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('x_save', english_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "word_lst_embeddings1 = np.load('/Users/baeyuna/Documents/SNU_DLab/TANR_Bert/data/glove/word_lst_embeddings1.npy')\n",
    "word_lst_embeddings10 = np.load('/Users/baeyuna/Documents/SNU_DLab/TANR_Bert/data/glove/word_lst_embeddings10.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(2000, 768)"
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "word_lst_embeddings10.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(2000, 768)"
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "word_lst_embeddings1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(4000, 768)"
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "source": [
    "np.concatenate([word_lst_embeddings1, word_lst_embeddings10]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(2000, 768)"
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "word_lst_embeddings1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[ 0.02291545,  0.01740933,  0.01044573, ...,  0.02647595,\n        -0.05486389,  0.001196  ],\n       [ 0.02087866,  0.03977818,  0.02973026, ..., -0.04849208,\n        -0.03428288, -0.03353313],\n       [-0.00754663,  0.00295051, -0.00115429, ..., -0.00839342,\n         0.01157734, -0.03211762],\n       ...,\n       [ 0.01743446, -0.05396994, -0.0726243 , ...,  0.00198761,\n        -0.03869714,  0.01757814],\n       [ 0.01157075,  0.03626885, -0.01124088, ...,  0.01655105,\n        -0.03771622, -0.04456491],\n       [-0.02233236, -0.0285336 , -0.02215027, ...,  0.01747822,\n        -0.04677428, -0.01103436]], dtype=float32)"
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "word_lst_embeddings1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "44775"
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "len(word_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "22"
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "44775//2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tf.Tensor: shape=(3, 768), dtype=float32, numpy=\narray([[-0.01112244,  0.00602975, -0.00272447, ...,  0.0336603 ,\n        -0.02208618, -0.05659664],\n       [-0.04087604, -0.01662398, -0.04726062, ...,  0.03640377,\n         0.02879217, -0.03352933],\n       [-0.01007188, -0.01358392, -0.03919495, ...,  0.0337332 ,\n        -0.01138729, -0.05158317]], dtype=float32)>"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "word_lst_embeddings = encode(word_lst)\n",
    "word_lst_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('D:/admin/Documents/x_save', word_lst_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "         0           1           2            3           4          5    \\\n0 -0.0111224  0.00602975 -0.00272447  0.000836123  -0.0263209  0.0247632   \n1  -0.040876   -0.016624  -0.0472606   -0.0340528  -0.0378542  0.0239878   \n2 -0.0100719  -0.0135839   -0.039195   -0.0589811  0.00538327    0.02524   \n\n         6           7          8          9    ...        758         759  \\\n0  0.0511221   0.0262025 -0.0621353 -0.0558861  ...  0.0116968 -0.00990636   \n1  0.0426077  0.00444569 -0.0612404 -0.0501724  ...  0.0101625  -0.0207745   \n2  0.0306217  -0.0189067 -0.0431255  0.0504395  ... -0.0536857  -0.0311351   \n\n          760        761         762         763        764        765  \\\n0   0.0337292  -0.014268  -0.0140283 -0.00477358 -0.0121225  0.0336603   \n1   0.0282323 -0.0286058 -0.00483788 -0.00685988  0.0179411  0.0364038   \n2  0.00727072  -0.038046   -0.023034   -0.015535 -0.0318728  0.0337332   \n\n         766        767  \n0 -0.0220862 -0.0565966  \n1  0.0287922 -0.0335293  \n2 -0.0113873 -0.0515832  \n\n[3 rows x 768 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>758</th>\n      <th>759</th>\n      <th>760</th>\n      <th>761</th>\n      <th>762</th>\n      <th>763</th>\n      <th>764</th>\n      <th>765</th>\n      <th>766</th>\n      <th>767</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-0.0111224</td>\n      <td>0.00602975</td>\n      <td>-0.00272447</td>\n      <td>0.000836123</td>\n      <td>-0.0263209</td>\n      <td>0.0247632</td>\n      <td>0.0511221</td>\n      <td>0.0262025</td>\n      <td>-0.0621353</td>\n      <td>-0.0558861</td>\n      <td>...</td>\n      <td>0.0116968</td>\n      <td>-0.00990636</td>\n      <td>0.0337292</td>\n      <td>-0.014268</td>\n      <td>-0.0140283</td>\n      <td>-0.00477358</td>\n      <td>-0.0121225</td>\n      <td>0.0336603</td>\n      <td>-0.0220862</td>\n      <td>-0.0565966</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-0.040876</td>\n      <td>-0.016624</td>\n      <td>-0.0472606</td>\n      <td>-0.0340528</td>\n      <td>-0.0378542</td>\n      <td>0.0239878</td>\n      <td>0.0426077</td>\n      <td>0.00444569</td>\n      <td>-0.0612404</td>\n      <td>-0.0501724</td>\n      <td>...</td>\n      <td>0.0101625</td>\n      <td>-0.0207745</td>\n      <td>0.0282323</td>\n      <td>-0.0286058</td>\n      <td>-0.00483788</td>\n      <td>-0.00685988</td>\n      <td>0.0179411</td>\n      <td>0.0364038</td>\n      <td>0.0287922</td>\n      <td>-0.0335293</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-0.0100719</td>\n      <td>-0.0135839</td>\n      <td>-0.039195</td>\n      <td>-0.0589811</td>\n      <td>0.00538327</td>\n      <td>0.02524</td>\n      <td>0.0306217</td>\n      <td>-0.0189067</td>\n      <td>-0.0431255</td>\n      <td>0.0504395</td>\n      <td>...</td>\n      <td>-0.0536857</td>\n      <td>-0.0311351</td>\n      <td>0.00727072</td>\n      <td>-0.038046</td>\n      <td>-0.023034</td>\n      <td>-0.015535</td>\n      <td>-0.0318728</td>\n      <td>0.0337332</td>\n      <td>-0.0113873</td>\n      <td>-0.0515832</td>\n    </tr>\n  </tbody>\n</table>\n<p>3 rows × 768 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "df = pd.DataFrame(index=range(0,word_lst_embeddings.shape[0]), columns=range(0,word_lst_embeddings.shape[1]))\n",
    "\n",
    "for i in range(word_lst_embeddings.shape[0]):\n",
    "    df.iloc[i,:] = word_lst_embeddings[i]\n",
    "\n",
    "df.loc[:,'words'] = word_lst \n",
    "df = df.rename_axis('words').iloc[:,:-1]\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = word2int.merge(source_embedding,\n",
    "                            how='inner',\n",
    "                            left_index=True,\n",
    "                            right_index=True)\n",
    "merged.set_index('int', inplace=True)\n",
    "merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missed_index = np.setdiff1d(np.arange(len(word2int) + 1),\n",
    "                            merged.index.values)\n",
    "missed_embedding = pd.DataFrame(data=np.random.normal(\n",
    "    size=(len(missed_index), 300)))\n",
    "missed_embedding['int'] = missed_index\n",
    "missed_embedding.set_index('int', inplace=True)\n",
    "missed_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_embedding = pd.concat([merged, missed_embedding]).sort_index()\n",
    "np.save('pretrained_word_embedding.npy', final_embedding.values)\n",
    "\n",
    "print(\n",
    "    f'Rate of word missed in pretrained embedding: {(len(missed_index)-1)/len(word2int):.4f}'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "mind",
   "display_name": "mind"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}